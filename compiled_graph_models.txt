

# ========== trainers_accelerate.py ==========

# trainers.py

import torch
import torch.optim as optim
from tqdm import tqdm
import logging
import matplotlib.pyplot as plt
from pathlib import Path

from src.graph_models.models.graph_networks import (
    GraphConvolutionNetwork,
    GraphAttentionNetwork,
    GraphTransformer,
    MeshGraphNet
)
from src.graph_models.models.graph_autoencoders import (
    GraphConvolutionalAutoEncoder,
    GraphAttentionAutoEncoder,
    GraphTransformerAutoEncoder,
    MeshGraphAutoEncoder
)
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.multiscale.gnn import (
    SinglescaleGNN, 
    MultiscaleGNN, 
    TopkMultiscaleGNN
)

# Import accelerate library
from accelerate import Accelerator

def identify_model_type(model):
    """
    Identifies the type of the model and returns a string identifier.
    """
    if isinstance(model, GNN_TopK):
        return 'GNN_TopK'
    elif isinstance(model, TopkMultiscaleGNN):
        return 'TopkMultiscaleGNN'
    elif isinstance(model, SinglescaleGNN):
        return 'SinglescaleGNN'
    elif isinstance(model, MultiscaleGNN):
        return 'MultiscaleGNN'
    elif isinstance(model, MeshGraphNet):
        return 'MeshGraphNet'
    elif isinstance(model, MeshGraphAutoEncoder):
        return 'MeshGraphAutoEncoder'
    elif isinstance(model, GraphTransformer):
        return 'GraphTransformer'
    elif isinstance(model, GraphTransformerAutoEncoder):
        return 'GraphTransformerAutoEncoder'
    else:
        return 'UnknownModel'

class BaseTrainer:
    def __init__(self, model, dataloader, optimizer, scheduler=None, device='cpu', **kwargs):
        # Initialize the accelerator
        self.accelerator = Accelerator()
        
        # Identify and store the model type
        self.model_type = identify_model_type(model)
        logging.info(f"Identified model type: {self.model_type}")
        
        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.dataloader = dataloader
        self.device = self.accelerator.device  # Use accelerator's device
        
        self.start_epoch = 0
        self.nepochs = kwargs.get('nepochs', 100)
        self.save_checkpoint_every = kwargs.get('save_checkpoint_every', 10)
        self.results_folder = Path(kwargs.get('results_folder', './results'))
        self.results_folder.mkdir(parents=True, exist_ok=True)
        self.loss_history = []
        self.verbose = kwargs.get('verbose', False)

        # Create 'checkpoints' subfolder under results_folder
        self.checkpoints_folder = self.results_folder / 'checkpoints'
        self.checkpoints_folder.mkdir(parents=True, exist_ok=True)

        self.random_seed = kwargs.get('random_seed', 42)

        # Checkpoint
        self.checkpoint = kwargs.get('checkpoint', None)
        if self.checkpoint:
            self.load_checkpoint(self.checkpoint)

        # Prepare the model, optimizer, scheduler, and dataloader
        if self.scheduler:
            self.model, self.optimizer, self.scheduler, self.dataloader = self.accelerator.prepare(
                self.model, self.optimizer, self.scheduler, self.dataloader)
        else:
            self.model, self.optimizer, self.dataloader = self.accelerator.prepare(
                self.model, self.optimizer, self.dataloader)

    def train(self):
        logging.info("Starting training...")
        for epoch in range(self.start_epoch, self.nepochs):
            self.model.train()
            total_loss = 0
            # Adjust progress bar for distributed training
            if self.verbose and self.accelerator.is_main_process:
                progress_bar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.nepochs}")
            else:
                progress_bar = self.dataloader
            for data in progress_bar:
                # No need to move data to device; accelerator handles it
                self.optimizer.zero_grad()
                loss = self.train_step(data)
                # Use accelerator's backward method
                self.accelerator.backward(loss)
                self.optimizer.step()
                total_loss += loss.item()
                if self.verbose and self.accelerator.is_main_process:
                    progress_bar.set_postfix(loss=total_loss / len(self.dataloader))

            # Scheduler step
            if self.scheduler:
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                if self.verbose:
                    logging.info(f"Epoch {epoch+1}: Learning rate adjusted to {current_lr}")

            # Save loss history
            avg_loss = total_loss / len(self.dataloader)
            self.loss_history.append(avg_loss)
            if self.accelerator.is_main_process:
                logging.info(f'Epoch {epoch+1}/{self.nepochs}, Loss: {avg_loss:.4e}')

            # Save checkpoint
            if (epoch + 1) % self.save_checkpoint_every == 0 or (epoch + 1) == self.nepochs:
                self.save_checkpoint(epoch)

        # Plot loss convergence
        if self.accelerator.is_main_process:
            self.plot_loss_convergence()
            logging.info("Training complete!")

    def train_step(self, data):
        raise NotImplementedError("Subclasses should implement this method.")

    def save_checkpoint(self, epoch):
        # Unwrap the model to get the original model (not wrapped by accelerator)
        unwrapped_model = self.accelerator.unwrap_model(self.model)
        checkpoint_path = self.checkpoints_folder / f'model-{epoch}.pth'
        # Prepare checkpoint data
        checkpoint_data = {
            'model_state_dict': unwrapped_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'epoch': epoch + 1,  # Save the next epoch to resume from
            'random_seed': self.random_seed,
            'loss_history': self.loss_history,
        }
        # Use accelerator's save method
        self.accelerator.wait_for_everyone()
        if self.accelerator.is_main_process:
            self.accelerator.save(checkpoint_data, checkpoint_path)
            logging.info(f"Model checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path):
        logging.info(f"Loading checkpoint from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if self.scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.start_epoch = checkpoint['epoch']
        if 'random_seed' in checkpoint:
            self.random_seed = checkpoint['random_seed']
            logging.info(f"Using random seed from checkpoint: {self.random_seed}")
        if 'loss_history' in checkpoint:
            self.loss_history = checkpoint['loss_history']
        logging.info(f"Resumed training from epoch {self.start_epoch}")

    def plot_loss_convergence(self):
        if self.accelerator.is_main_process:
            plt.figure(figsize=(10, 6))
            plt.plot(self.loss_history, label="Training Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("Loss Convergence")
            plt.legend()
            plt.grid(True)
            plt.savefig(self.results_folder / "loss_convergence.png")
            plt.close()

class GraphPredictionTrainer(BaseTrainer):
    def __init__(self, criterion=None, **kwargs):
        # Pop out values from kwargs to prevent duplication
        model = kwargs.pop('model', None)
        dataloader = kwargs.pop('dataloader', None)
        optimizer = kwargs.pop('optimizer', None)
        scheduler = kwargs.pop('scheduler', None)
        device = kwargs.pop('device', 'cpu')
        
        # Pass the remaining kwargs to the parent class
        super().__init__(model=model,
                         dataloader=dataloader,
                         optimizer=optimizer,
                         scheduler=scheduler,
                         device=device,
                         **kwargs)
        
        # Set the loss function
        if criterion is not None:
            self.criterion = criterion
        else:
            # Default loss function for regression tasks
            self.criterion = torch.nn.MSELoss()
        
        logging.info(f"Using loss function: {self.criterion.__class__.__name__}")

    def train_step(self, data):
        # # Check edge_attr
        # if not hasattr(data, 'edge_attr') or data.edge_attr is None:
        #     logging.error("data.edge_attr is missing or None.")
        #     raise ValueError("data.edge_attr is missing or None.")
        # else:
        #     logging.info(f"data.edge_attr shape: {data.edge_attr.shape}")
        
        x_pred = self.model_forward(data)
        loss = self.criterion(x_pred, data.y)
        return loss

    def model_forward(self, data):
        """
        Calls the model's forward method based on the identified model type.
        """
        model_type = self.model_type
        if model_type == 'GNN_TopK':
            x_pred, _ = self.model(
                x=data.x,
                edge_index=data.edge_index,
                edge_attr=data.edge_attr,
                pos=data.pos,
                batch=data.batch
            )
        elif model_type == 'TopkMultiscaleGNN':
            x_pred, mask = self.model(
                x=data.x,
                edge_index=data.edge_index,
                pos=data.pos,
                edge_attr=data.edge_attr,
                batch=data.batch
            )
        elif model_type in ['SinglescaleGNN', 'MultiscaleGNN']:
            x_pred = self.model(
                x=data.x,
                edge_index=data.edge_index,
                pos=data.pos,
                edge_attr=data.edge_attr,
                batch=data.batch
            )
        elif model_type in ['MeshGraphNet', 'MeshGraphAutoEncoder']:
            # MeshGraphNet and MeshGraphAutoEncoder use edge attributes
            x_pred = self.model(
                x=data.x,
                edge_index=data.edge_index,
                edge_attr=data.edge_attr,
                batch=data.batch  # Removed 'u=None'
            )
        elif model_type in ['GraphTransformer', 'GraphTransformerAutoEncoder']:
            x_pred = self.model(
                x=data.x,
                edge_index=data.edge_index,
                edge_attr=data.edge_attr if hasattr(data, 'edge_attr') else None,
                batch=data.batch
            )
        else:  # GraphConvolutionNetwork, GraphAttentionNetwork, etc. TODO: Add specific model types here
            x_pred = self.model(
                x=data.x,
                edge_index=data.edge_index,
                batch=data.batch
            )
        return x_pred


# ========== evaluate_checkpoint.py ==========

# # Filename: evaluate_checkpoint.py

# import argparse
# import os
# import re
# import torch
# import torch.nn.functional as F
# import numpy as np
# import json
# import matplotlib.pyplot as plt
# from torch.utils.data import Dataset, Subset
# from torch_geometric.data import Data
# from torch_geometric.loader import DataLoader  # Use PyTorch Geometric's DataLoader
# from tqdm import tqdm
# from pmd_beamphysics import ParticleGroup  # Import ParticleGroup
# import logging
# import sys

# # Set up logging
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# # Import your models and utilities
# from datasets import GraphDataset
# from utils import generate_data_dirs, set_random_seed
# from config import parse_args as parse_train_args
# from src.graph_models.models.graph_networks import (
#     GraphConvolutionNetwork,
#     GraphAttentionNetwork,
#     GraphTransformer,
#     MeshGraphNet
# )
# from src.graph_models.models.graph_autoencoders import (
#     GraphConvolutionalAutoEncoder,
#     GraphAttentionAutoEncoder,
#     GraphTransformerAutoEncoder,
#     MeshGraphAutoEncoder
# )
# from src.graph_models.models.intgnn.models import GNN_TopK
# from src.graph_models.models.multiscale.gnn import (
#     SinglescaleGNN, 
#     MultiscaleGNN, 
#     TopkMultiscaleGNN
# )
# from trainers import GraphPredictionTrainer

# def parse_hyperparameters_from_folder_name(folder_name):
#     """
#     Parses hyperparameters from the folder name and returns them as a dictionary.
#     """
#     hyperparams = {}
#     # Split the folder name into parts
#     parts = folder_name.split('_')
#     # The data_keyword may contain underscores, so we need to handle that
#     # Let's assume that known hyperparameter keys have specific prefixes
#     known_prefixes = {'r', 'nt', 'b', 'lr', 'h', 'ly', 'pr', 'ep', 'sch', 'heads', 'concat', 'dropout', 'mlph', 'mmply', 'mply'}
#     idx = 0
#     # Collect data_keyword parts until we find a part starting with a known prefix
#     data_keyword_parts = []
#     while idx < len(parts) and not any(parts[idx].startswith(prefix) for prefix in known_prefixes):
#         data_keyword_parts.append(parts[idx])
#         idx += 1
#     hyperparams['data_keyword'] = '_'.join(data_keyword_parts)
#     # Now parse the remaining parts
#     while idx < len(parts):
#         part = parts[idx]
#         matched = False
#         for prefix in known_prefixes:
#             if part.startswith(prefix):
#                 value = part[len(prefix):]
#                 if prefix == 'pr':  # pool_ratios
#                     # Pool ratios may be multiple numbers separated by '_'
#                     ratios = value.split('_')
#                     hyperparams['pool_ratios'] = [float(r) for r in ratios]
#                 elif prefix == 'concat':
#                     hyperparams['gtr_concat'] = True if value == 'True' else False
#                 elif prefix == 'sch':
#                     # Handle scheduler
#                     hyperparams['lr_scheduler'] = value
#                     # For 'sch_lin_100_1000_1e-05', extract additional parameters
#                     if value.startswith('lin'):
#                         lin_params = parts[idx + 1:idx + 4]
#                         if len(lin_params) >= 3:
#                             hyperparams['lin_start_epoch'] = int(lin_params[0])
#                             hyperparams['lin_end_epoch'] = int(lin_params[1])
#                             hyperparams['lin_final_lr'] = float(lin_params[2])
#                             idx += 3  # Skip the parameters we just consumed
#                 else:
#                     param_name = {
#                         'r': 'random_seed',
#                         'nt': 'ntrain',
#                         'b': 'batch_size',
#                         'lr': 'lr',
#                         'h': 'hidden_dim',
#                         'ly': 'num_layers',
#                         'ep': 'nepochs',
#                         'heads': 'heads',
#                         'dropout': 'gtr_dropout',
#                         'mlph': 'multiscale_n_mlp_hidden_layers',
#                         'mmply': 'multiscale_n_mmp_layers',
#                         'mply': 'multiscale_n_message_passing_layers',
#                     }.get(prefix, prefix)
#                     hyperparams[param_name] = type_cast(param_name, value)
#                 matched = True
#                 break
#         if not matched:
#             logging.warning(f"Unrecognized hyperparameter part: {part}")
#         idx += 1
#     return hyperparams

# def extract_hyperparameters_from_checkpoint(checkpoint_path):
#     """
#     Extracts hyperparameters from the checkpoint path.
#     """
#     # Split the path
#     path_parts = checkpoint_path.split(os.sep)
#     # Remove any empty strings from leading/trailing slashes
#     path_parts = [part for part in path_parts if part]
#     if 'checkpoints' not in path_parts:
#         logging.error("'checkpoints' directory not found in the checkpoint path.")
#         sys.exit(1)
#     # Find the index of 'checkpoints'
#     checkpoint_idx = path_parts.index('checkpoints')
#     if checkpoint_idx < 4:
#         logging.error("Checkpoint path is too short to extract hyperparameters.")
#         sys.exit(1)
#     # Extract components
#     folder_name = path_parts[checkpoint_idx - 1]
#     task = path_parts[checkpoint_idx - 2]
#     dataset = path_parts[checkpoint_idx - 3]
#     model = path_parts[checkpoint_idx - 4]
#     base_results_dir = os.sep + os.sep.join(path_parts[:checkpoint_idx - 4])
#     # Parse hyperparameters from folder name
#     hyperparams = parse_hyperparameters_from_folder_name(folder_name)
#     # Add model, dataset, task to hyperparameters
#     hyperparams['model'] = model
#     hyperparams['dataset'] = dataset
#     hyperparams['task'] = task
#     hyperparams['base_results_dir'] = base_results_dir
#     return hyperparams


# def type_cast(key, value):
#     """
#     Helper function to cast hyperparameter values to appropriate types.
#     """
#     int_params = {
#         'random_seed', 'ntrain', 'batch_size', 'hidden_dim', 'num_layers', 'nepochs',
#         'gat_heads', 'gtr_heads',
#         'multiscale_n_mlp_hidden_layers',
#         'multiscale_n_mmp_layers',
#         'multiscale_n_message_passing_layers'
#     }
#     float_params = {'lr', 'gtr_dropout'}
#     if key in int_params:
#         return int(value)
#     elif key in float_params:
#         return float(value)
#     else:
#         return value

# # # Function to extract hyperparameters from the checkpoint path
# # def extract_hyperparameters_from_checkpoint(checkpoint_path):
# #     """
# #     Extracts hyperparameters from the checkpoint path.
# #     """
# #     # Expected checkpoint path format:
# #     # base_results_dir/model/dataset/task/folder_name/checkpoints/model-epoch.pth
# #     # Split the path
# #     path_parts = checkpoint_path.split(os.sep)
# #     if len(path_parts) < 5:
# #         logging.error("Checkpoint path is too short to extract hyperparameters.")
# #         sys.exit(1)
# #     # Extract components
# #     base_results_dir = os.sep.join(path_parts[:-5])
# #     model = path_parts[-5]
# #     dataset = path_parts[-4]
# #     task = path_parts[-3]
# #     folder_name = path_parts[-2]
# #     # Parse hyperparameters from folder name
# #     hyperparams = parse_hyperparameters_from_folder_name(folder_name)
# #     # Add model, dataset, task to hyperparameters
# #     hyperparams['model'] = model
# #     hyperparams['dataset'] = dataset
# #     hyperparams['task'] = task
# #     hyperparams['base_results_dir'] = base_results_dir
# #     return hyperparams

# # Function to check if any required hyperparameters are missing
# def check_missing_hyperparameters(hyperparams, required_params):
#     missing_params = []
#     for param in required_params:
#         if param not in hyperparams:
#             missing_params.append(param)
#     if missing_params:
#         logging.error(f"Missing hyperparameters: {missing_params}")
#         sys.exit(1)

# # Function to transform data into ParticleGroup
# def transform_to_particle_group(data):
#     """
#     Converts data tensor to ParticleGroup.

#     Args:
#         data (torch.Tensor): Tensor of shape [num_nodes, 6]

#     Returns:
#         ParticleGroup
#     """
#     num_particles = data.shape[0]
    
#     particle_dict = {
#         'x': data[:, 0].numpy(),
#         'y': data[:, 1].numpy(),
#         'z': data[:, 2].numpy(),
#         'px': data[:, 3].numpy(),
#         'py': data[:, 4].numpy(),
#         'pz': data[:, 5].numpy(),
#         'species': 'electron',
#         'weight': 2.e-17 * np.ones(num_particles),
#         't': np.zeros(num_particles),  # Set 't' to zeros
#         'status': [1] * num_particles
#     }
#     particle_group = ParticleGroup(data=particle_dict)
#     return particle_group

# # def compute_normalized_emittance_x(particle_group):
# #     """
# #     Computes the normalized emittance in x direction for a ParticleGroup.

# #     Args:
# #         particle_group (ParticleGroup): The ParticleGroup for which to compute the emittance.

# #     Returns:
# #         float: The normalized emittance in x direction.
# #     """
# #     return particle_group['norm_emit_x']

# def compute_normalized_emittance_x(particle_group):
#     x = particle_group['x']
#     px = particle_group['px']
#     # Calculate the statistical moments
#     mean_x2 = np.mean(x**2)
#     mean_px2 = np.mean(px**2)
#     mean_xpx = np.mean(x * px)
    
#     # Compute the norm emittance
#     norm_emit_x = np.sqrt(mean_x2 * mean_px2 - mean_xpx**2)
    
#     return norm_emit_x

# # Function to plot ParticleGroups
# def plot_particle_groups(pred_pg, target_pg, idx, error_type, results_folder):
#     """
#     Plots and saves figures for predicted and target ParticleGroups.

#     Args:
#         pred_pg (ParticleGroup): Predicted ParticleGroup
#         target_pg (ParticleGroup): Target ParticleGroup
#         idx (int): Sample index
#         error_type (str): 'min' or 'max'
#         results_folder (str): Folder to save figures
#     """
#     for var_pair in [('x', 'px'), ('y', 'py'), ('z', 'pz')]:
#         x_var, p_var = var_pair
#         # Plot predicted
#         plt.figure(figsize=(6,6))
#         pred_pg.plot(x_var, p_var, label='Predicted', alpha=0.6)
#         # plt.legend()
#         plt.grid(True)
#         # plt.title(f'Sample {idx}: Predicted {x_var} vs {p_var}')
#         plt.savefig(os.path.join(results_folder, f'{error_type}_mse_sample_{idx}_pred_{x_var}_{p_var}.png'))
#         plt.close()
#         # Plot target
#         plt.figure(figsize=(6,6))
#         target_pg.plot(x_var, p_var, label='Target', alpha=0.6)
#         # plt.legend()
#         plt.grid(True)
#         # plt.title(f'Sample {idx}: Target {x_var} vs {p_var}')
#         plt.savefig(os.path.join(results_folder, f'{error_type}_mse_sample_{idx}_target_{x_var}_{p_var}.png'))
#         plt.close()


# # Function to evaluate the model
# def evaluate_model(model, dataloader, device, metadata_final_path, results_folder):
#     model.eval()
#     all_errors = []
#     all_predictions = []
#     all_targets = []
#     with torch.no_grad():
#         for data in tqdm(dataloader, desc="Evaluating Model"):
#             data = data.to(device)
#             # x_pred = model(data.x, data.edge_index, data.batch)
#             x_pred = model_forward(model, data)
#             # Compute MSE per sample
#             mse = F.mse_loss(x_pred, data.y, reduction='none')  # Shape: [num_nodes, features]
#             mse = mse.mean(dim=1)  # Mean over features: [num_nodes]
#             # Now aggregate MSE per graph
#             batch_indices = data.batch.cpu().numpy()
#             graph_indices = np.unique(batch_indices)
#             for idx in graph_indices:
#                 mask = (batch_indices == idx)
#                 graph_mse = mse[mask].mean().item()
#                 all_errors.append(graph_mse)
#                 # Store predictions and targets for this graph
#                 all_predictions.append(x_pred[mask].cpu())
#                 all_targets.append(data.y[mask].cpu())
#     # Now we have all_errors, all_predictions, all_targets
#     all_errors = np.array(all_errors)
#     # Find indices of 5 minimal MSE and 5 maximal MSE
#     sorted_indices = np.argsort(all_errors)
#     min_mse_indices = sorted_indices[:5]
#     max_mse_indices = sorted_indices[-5:]
#     # Load metadata_final.json
#     with open(metadata_final_path, 'r') as f:
#         metadata_final = json.load(f)
#     global_mean_final = torch.tensor(metadata_final['global_mean'])
#     global_std_final = torch.tensor(metadata_final['global_std'])
#     # Define inverse normalization function
#     def inverse_normalize(normalized_data):
#         return normalized_data * global_std_final + global_mean_final
#     # For the selected samples, produce figures
#     # For min MSE samples
#     min_mse_relative_errors = []
#     for idx in min_mse_indices:
#         pred = all_predictions[idx]
#         target = all_targets[idx]
#         # Inverse normalize
#         pred_original = inverse_normalize(pred)
#         target_original = inverse_normalize(target)
#         # Convert to ParticleGroup
#         pred_pg = transform_to_particle_group(pred_original)
#         target_pg = transform_to_particle_group(target_original)
#         # Plot and save figures
#         plot_particle_groups(pred_pg, target_pg, idx, 'min', results_folder)
        
#         # Compute normalized emittances
#         pred_norm_emit_x = compute_normalized_emittance_x(pred_pg)
#         target_norm_emit_x = compute_normalized_emittance_x(target_pg)
#         # Compute relative error
#         relative_error = abs(pred_norm_emit_x - target_norm_emit_x) / abs(target_norm_emit_x)
#         min_mse_relative_errors.append(relative_error)
#         # Print the relative error
#         print(f"Sample {idx} (min MSE): Relative Error in norm_emit_x: {relative_error}")
    
#     # For max MSE samples
#     max_mse_relative_errors = []
#     for idx in max_mse_indices:
#         pred = all_predictions[idx]
#         target = all_targets[idx]
#         # Inverse normalize
#         pred_original = inverse_normalize(pred)
#         target_original = inverse_normalize(target)
#         # Convert to ParticleGroup
#         pred_pg = transform_to_particle_group(pred_original)
#         target_pg = transform_to_particle_group(target_original)
#         # Plot and save figures
#         plot_particle_groups(pred_pg, target_pg, idx, 'max', results_folder)
        
#         # Compute normalized emittances
#         pred_norm_emit_x = compute_normalized_emittance_x(pred_pg)
#         target_norm_emit_x = compute_normalized_emittance_x(target_pg)
#         # Compute relative error
#         relative_error = abs(pred_norm_emit_x - target_norm_emit_x) / abs(target_norm_emit_x)
#         max_mse_relative_errors.append(relative_error)
#         # Print the relative error
#         print(f"Sample {idx} (max MSE): Relative Error in norm_emit_x: {relative_error}")
#     # Compute overall test error
#     test_error = all_errors.mean()
#     print(f"Test Error (MSE): {test_error}")

# def model_forward(model, data):
#     """
#     Forward pass for the model, handling different model types.
#     """
#     if isinstance(model, GNN_TopK):
#         x_pred, _ = model(
#             data.x,
#             data.edge_index,
#             data.edge_attr,
#             data.pos,
#             batch=data.batch
#         )
#     elif isinstance(model, TopkMultiscaleGNN):
#         x_pred, mask = model(
#             data.x,
#             data.edge_index,
#             data.pos,
#             data.edge_attr,
#             data.batch
#         )
#     elif isinstance(model, SinglescaleGNN) or isinstance(model, MultiscaleGNN):
#         x_pred = model(
#             data.x,
#             data.edge_index,
#             data.pos,
#             data.edge_attr,
#             data.batch
#         )
#     elif isinstance(model, MeshGraphNet) or isinstance(model, MeshGraphAutoEncoder):
#         # MeshGraphNet uses edge attributes
#         x_pred = model(
#             data.x,
#             data.edge_index,
#             data.edge_attr,
#             data.batch
#         )
#     elif isinstance(model, GraphTransformer) or isinstance(model, GraphTransformerAutoEncoder):
#         x_pred = model(
#             data.x,
#             data.edge_index,
#             data.edge_attr if hasattr(data, 'edge_attr') else None,
#             data.batch
#         )
#     else:  # GraphConvolutionNetwork, GraphAttentionNetwork
#         x_pred = model(
#             data.x,
#             data.edge_index,
#             data.batch
#         )
#     return x_pred

# # Main function
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Evaluate a model checkpoint")
#     parser.add_argument('--checkpoint', type=str, required=True, help="Path to the model checkpoint")
#     parser.add_argument('--cpu_only', action='store_true', help="Force the script to use CPU even if GPU is available")
#     parser.add_argument('--results_folder', type=str, default='evaluation_results', help="Folder to save evaluation results")
#     parser.add_argument('--subsample_size', type=int, default=None, help="Number of samples to use for evaluation (if None, use all)")
#     args = parser.parse_args()

#     # Set device
#     if args.cpu_only:
#         device = torch.device('cpu')
#     else:
#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     logging.info(f"Using device: {device}")

#     # Extract hyperparameters from the checkpoint path
#     hyperparams = extract_hyperparameters_from_checkpoint(args.checkpoint)
#     logging.info(f"Extracted hyperparameters: {hyperparams}")

#     # Define required hyperparameters
#     required_hyperparams = [
#         'model', 'dataset', 'task', 'data_keyword', 'random_seed', 'batch_size',
#         'hidden_dim', 'num_layers', 'pool_ratios'
#         # Add more required parameters as needed
#     ]

#     # Check for missing hyperparameters
#     check_missing_hyperparameters(hyperparams, required_params=required_hyperparams)

#     # Set random seed
#     set_random_seed(hyperparams['random_seed'])

#     # Generate data directories
#     initial_graph_dir, final_graph_dir, settings_dir = generate_data_dirs(
#         hyperparams.get('base_data_dir', '/sdf/data/ad/ard/u/tiffan/data/'),
#         hyperparams['dataset'],
#         hyperparams['data_keyword']
#     )
#     logging.info(f"Initial graph directory: {initial_graph_dir}")
#     logging.info(f"Final graph directory: {final_graph_dir}")
#     logging.info(f"Settings directory: {settings_dir}")

#     # Initialize dataset
#     use_edge_attr = hyperparams['model'].lower() in [
#         'intgnn', 'gtr', 'mgn', 'gtr-ae', 'mgn-ae', 
#         'singlescale', 'multiscale', 'multiscale-topk'
#     ]
#     logging.info(f"Model '{hyperparams['model']}' requires edge_attr: {use_edge_attr}")

#     dataset = GraphDataset(
#         initial_graph_dir=initial_graph_dir,
#         final_graph_dir=final_graph_dir,
#         settings_dir=settings_dir,
#         task=hyperparams['task'],
#         use_edge_attr=use_edge_attr
#     )

#     total_dataset_size = len(dataset)
#     logging.info(f"Total dataset size: {total_dataset_size}")

#     # Subset dataset if subsample_size is specified
#     if args.subsample_size is not None:
#         np.random.seed(hyperparams['random_seed'])  # For reproducibility
#         if args.subsample_size >= total_dataset_size:
#             logging.warning(f"Requested subsample_size {args.subsample_size} is greater than or equal to total dataset size {total_dataset_size}. Using full dataset.")
#         else:
#             indices = np.random.permutation(total_dataset_size)[:args.subsample_size]
#             dataset = Subset(dataset, indices)
#             logging.info(f"Using a subsample of {args.subsample_size} samples for evaluation.")

#     dataloader = DataLoader(dataset, batch_size=hyperparams['batch_size'], shuffle=False)

#     # # Subset dataset if ntrain is specified
#     # total_dataset_size = len(dataset)
#     # if hyperparams.get('ntrain') is not None:
#     #     np.random.seed(hyperparams['random_seed'])  # For reproducibility
#     #     indices = np.random.permutation(total_dataset_size)[:hyperparams['ntrain']]
#     #     dataset = Subset(dataset, indices)

#     # dataloader = DataLoader(dataset, batch_size=hyperparams['batch_size'], shuffle=False)

#     # Get a sample data for model initialization
#     sample = dataset[0]

#     # Model initialization
#     # Reuse the model initialization code from train.py, adjusting as necessary
#     # You may need to define a function to initialize the model based on hyperparameters

#     # Here, we'll define a function to initialize the model
#     def initialize_model(hyperparams, sample):
#         model_name = hyperparams['model'].lower()

#         def is_autoencoder_model(model_name):
#             return model_name.lower().endswith('-ae') or model_name.lower() in ['multiscale-topk']

#         if is_autoencoder_model(model_name):
#             # Autoencoder models
#             num_layers = hyperparams['num_layers']
#             if num_layers % 2 != 0:
#                 raise ValueError(f"For autoencoder models, 'num_layers' must be an even number. Received: {num_layers}")
#             depth = num_layers // 2
#             logging.info(f"Autoencoder selected. Using depth: {depth} (num_layers: {num_layers})")
#             required_pool_ratios = depth - 1
#             pool_ratios = hyperparams.get('pool_ratios', [])
#             current_pool_ratios = len(pool_ratios)

#             if required_pool_ratios <= 0:
#                 pool_ratios = []
#                 logging.info(f"No pooling layers required for depth {depth}.")
#             elif current_pool_ratios < required_pool_ratios:
#                 pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
#                 logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
#             elif current_pool_ratios > required_pool_ratios:
#                 pool_ratios = pool_ratios[:required_pool_ratios]
#                 logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

#             if model_name == 'gcn-ae':
#                 in_channels = sample.x.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]

#                 model = GraphConvolutionalAutoEncoder(
#                     in_channels=in_channels,
#                     hidden_dim=hidden_dim,
#                     out_channels=out_channels,
#                     depth=depth,
#                     pool_ratios=pool_ratios
#                 )
#                 logging.info("Initialized GraphConvolutionalAutoEncoder.")

#             elif model_name == 'gat-ae':
#                 in_channels = sample.x.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]
#                 heads = hyperparams.get('gat_heads', 1)

#                 model = GraphAttentionAutoEncoder(
#                     in_channels=in_channels,
#                     hidden_dim=hidden_dim,
#                     out_channels=out_channels,
#                     depth=depth,
#                     pool_ratios=pool_ratios,
#                     heads=heads
#                 )
#                 logging.info("Initialized GraphAttentionAutoEncoder.")

#             elif model_name == 'gtr-ae':
#                 in_channels = sample.x.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]
#                 num_heads = hyperparams.get('gtr_heads', 4)
#                 concat = hyperparams.get('gtr_concat', True)
#                 dropout = hyperparams.get('gtr_dropout', 0.0)
#                 edge_dim = sample.edge_attr.shape[1] if sample.edge_attr is not None else None

#                 model = GraphTransformerAutoEncoder(
#                     in_channels=in_channels,
#                     hidden_dim=hidden_dim,
#                     out_channels=out_channels,
#                     depth=depth,
#                     pool_ratios=pool_ratios,
#                     num_heads=num_heads,
#                     concat=concat,
#                     dropout=dropout,
#                     edge_dim=edge_dim
#                 )
#                 logging.info("Initialized GraphTransformerAutoEncoder.")

#             elif model_name == 'mgn-ae':
#                 node_in_dim = sample.x.shape[1]
#                 edge_in_dim = sample.edge_attr.shape[1] if sample.edge_attr is not None else 0
#                 node_out_dim = sample.y.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']

#                 model = MeshGraphAutoEncoder(
#                     node_in_dim=node_in_dim,
#                     edge_in_dim=edge_in_dim,
#                     node_out_dim=node_out_dim,
#                     hidden_dim=hidden_dim,
#                     depth=depth,
#                     pool_ratios=pool_ratios
#                 )
#                 logging.info("Initialized MeshGraphAutoEncoder.")

#             elif model_name == 'multiscale-topk':
#                 input_node_channels = sample.x.shape[1]
#                 input_edge_channels = sample.edge_attr.shape[1] if sample.edge_attr is not None else 0
#                 hidden_channels = hyperparams['hidden_dim']
#                 output_node_channels = sample.y.shape[1]
#                 n_mlp_hidden_layers = hyperparams.get('multiscale_n_mlp_hidden_layers', 2)
#                 n_mmp_layers = hyperparams.get('multiscale_n_mmp_layers', 4)
#                 n_messagePassing_layers = hyperparams.get('multiscale_n_message_passing_layers', 2)
#                 max_level_mmp = num_layers // 2 - 1
#                 max_level_topk = num_layers // 2 - 1

#                 # Compute l_char (characteristic length scale)
#                 edge_index = sample.edge_index
#                 pos = sample.pos
#                 edge_lengths = torch.norm(pos[edge_index[0]] - pos[edge_index[1]], dim=1)
#                 l_char = edge_lengths.mean().item()
#                 logging.info(f"Computed l_char (characteristic length scale): {l_char}")

#                 name = 'topk_multiscale_gnn'

#                 model = TopkMultiscaleGNN(
#                     input_node_channels=input_node_channels,
#                     input_edge_channels=input_edge_channels,
#                     hidden_channels=hidden_channels,
#                     output_node_channels=output_node_channels,
#                     n_mlp_hidden_layers=n_mlp_hidden_layers,
#                     n_mmp_layers=n_mmp_layers,
#                     n_messagePassing_layers=n_messagePassing_layers,
#                     max_level_mmp=max_level_mmp,
#                     max_level_topk=max_level_topk,
#                     pool_ratios=pool_ratios,
#                     l_char=l_char,
#                     name=name
#                 )
#                 logging.info("Initialized TopkMultiscaleGNN model.")

#             else:
#                 raise ValueError(f"Unknown autoencoder model {model_name}")

#         else:
#             # Non-autoencoder models
#             if model_name == 'intgnn':
#                 in_channels_node = sample.x.shape[1]
#                 in_channels_edge = sample.edge_attr.shape[1] if sample.edge_attr is not None else 0
#                 hidden_channels = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]
#                 n_mlp_encode = 3
#                 n_mlp_mp = 2
#                 n_mp_down_topk = [1, 1]
#                 n_mp_up_topk = [1, 1]
#                 pool_ratios = hyperparams.get('pool_ratios', [])
#                 n_mp_down_enc = [4]
#                 n_mp_up_enc = []
#                 lengthscales_enc = []
#                 n_mp_down_dec = [2, 2, 4]
#                 n_mp_up_dec = [2, 2]
#                 lengthscales_dec = [0.5, 1.0]
#                 interp = 'learned'
#                 act = F.elu
#                 param_sharing = False

#                 # Create bounding box if needed
#                 bounding_box = []
#                 if len(lengthscales_dec) > 0:
#                     x_lo = sample.pos[:, 0].min() - lengthscales_dec[0] / 2
#                     x_hi = sample.pos[:, 0].max() + lengthscales_dec[0] / 2
#                     y_lo = sample.pos[:, 1].min() - lengthscales_dec[0] / 2
#                     y_hi = sample.pos[:, 1].max() + lengthscales_dec[0] / 2
#                     z_lo = sample.pos[:, 2].min() - lengthscales_dec[0] / 2
#                     z_hi = sample.pos[:, 2].max() + lengthscales_dec[0] / 2
#                     bounding_box = [
#                         x_lo.item(), x_hi.item(),
#                         y_lo.item(), y_hi.item(),
#                         z_lo.item(), z_hi.item()
#                     ]

#                 model = GNN_TopK(
#                     in_channels_node,
#                     in_channels_edge,
#                     hidden_channels,
#                     out_channels,
#                     n_mlp_encode,
#                     n_mlp_mp,
#                     n_mp_down_topk,
#                     n_mp_up_topk,
#                     pool_ratios,
#                     n_mp_down_enc,
#                     n_mp_up_enc,
#                     n_mp_down_dec,
#                     n_mp_up_dec,
#                     lengthscales_enc,
#                     lengthscales_dec,
#                     bounding_box,
#                     interp,
#                     act,
#                     param_sharing,
#                     name='gnn_topk'
#                 )
#                 logging.info("Initialized GNN_TopK model.")

#             elif model_name == 'singlescale':
#                 input_node_channels = sample.x.shape[1]
#                 input_edge_channels = sample.edge_attr.shape[1] if sample.edge_attr is not None else 0
#                 hidden_channels = hyperparams['hidden_dim']
#                 output_node_channels = sample.y.shape[1]
#                 n_mlp_hidden_layers = 0  # As per MeshGraphNet
#                 n_messagePassing_layers = hyperparams['num_layers']
#                 name = 'singlescale_gnn'

#                 model = SinglescaleGNN(
#                     input_node_channels=input_node_channels,
#                     input_edge_channels=input_edge_channels,
#                     hidden_channels=hidden_channels,
#                     output_node_channels=output_node_channels,
#                     n_mlp_hidden_layers=n_mlp_hidden_layers,
#                     n_messagePassing_layers=n_messagePassing_layers,
#                     name=name
#                 )
#                 logging.info("Initialized SinglescaleGNN model.")

#             elif model_name == 'multiscale':
#                 input_node_channels = sample.x.shape[1]
#                 input_edge_channels = sample.edge_attr.shape[1] if sample.edge_attr is not None else 0
#                 hidden_channels = hyperparams['hidden_dim']
#                 output_node_channels = sample.y.shape[1]
#                 n_mlp_hidden_layers = hyperparams.get('multiscale_n_mlp_hidden_layers', 2)
#                 n_mmp_layers = hyperparams.get('multiscale_n_mmp_layers', 4)
#                 n_messagePassing_layers = hyperparams.get('multiscale_n_message_passing_layers', 2)
#                 num_layers = hyperparams['num_layers']
#                 max_level = num_layers // 2 - 1

#                 # Compute l_char (characteristic length scale)
#                 edge_index = sample.edge_index
#                 pos = sample.pos
#                 edge_lengths = torch.norm(pos[edge_index[0]] - pos[edge_index[1]], dim=1)
#                 l_char = edge_lengths.mean().item()
#                 logging.info(f"Computed l_char (characteristic length scale): {l_char}")

#                 name = 'multiscale_gnn'

#                 model = MultiscaleGNN(
#                     input_node_channels=input_node_channels,
#                     input_edge_channels=input_edge_channels,
#                     hidden_channels=hidden_channels,
#                     output_node_channels=output_node_channels,
#                     n_mlp_hidden_layers=n_mlp_hidden_layers,
#                     n_mmp_layers=n_mmp_layers,
#                     n_messagePassing_layers=n_messagePassing_layers,
#                     max_level=max_level,
#                     l_char=l_char,
#                     name=name
#                 )
#                 logging.info("Initialized MultiscaleGNN model.")

#             elif model_name == 'gcn':
#                 num_layers = hyperparams['num_layers']
#                 required_pool_ratios = num_layers - 2
#                 pool_ratios = hyperparams.get('pool_ratios', [])
#                 current_pool_ratios = len(pool_ratios)

#                 if required_pool_ratios <= 0:
#                     pool_ratios = []
#                     logging.info(f"No pooling layers required for num_layers {num_layers}.")
#                 elif current_pool_ratios < required_pool_ratios:
#                     pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
#                     logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
#                 elif current_pool_ratios > required_pool_ratios:
#                     pool_ratios = pool_ratios[:required_pool_ratios]
#                     logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

#                 in_channels = sample.x.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]

#                 model = GraphConvolutionNetwork(
#                     in_channels=in_channels,
#                     hidden_dim=hidden_dim,
#                     out_channels=out_channels,
#                     num_layers=num_layers,
#                     pool_ratios=pool_ratios,
#                 )
#                 logging.info("Initialized GraphConvolutionNetwork model.")

#             elif model_name == 'gat':
#                 num_layers = hyperparams['num_layers']
#                 required_pool_ratios = num_layers - 2
#                 pool_ratios = hyperparams.get('pool_ratios', [])
#                 current_pool_ratios = len(pool_ratios)

#                 if required_pool_ratios <= 0:
#                     pool_ratios = []
#                     logging.info(f"No pooling layers required for num_layers {num_layers}.")
#                 elif current_pool_ratios < required_pool_ratios:
#                     pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
#                     logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
#                 elif current_pool_ratios > required_pool_ratios:
#                     pool_ratios = pool_ratios[:required_pool_ratios]
#                     logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

#                 in_channels = sample.x.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]
#                 heads = hyperparams.get('gat_heads', 1)

#                 model = GraphAttentionNetwork(
#                     in_channels=in_channels,
#                     hidden_dim=hidden_dim,
#                     out_channels=out_channels,
#                     num_layers=num_layers,
#                     pool_ratios=pool_ratios,
#                     heads=heads,
#                 )
#                 logging.info("Initialized GraphAttentionNetwork model.")

#             elif model_name == 'gtr':
#                 num_layers = hyperparams['num_layers']
#                 required_pool_ratios = num_layers - 2
#                 pool_ratios = hyperparams.get('pool_ratios', [])
#                 current_pool_ratios = len(pool_ratios)

#                 if required_pool_ratios <= 0:
#                     pool_ratios = []
#                     logging.info(f"No pooling layers required for num_layers {num_layers}.")
#                 elif current_pool_ratios < required_pool_ratios:
#                     pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
#                     logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
#                 elif current_pool_ratios > required_pool_ratios:
#                     pool_ratios = pool_ratios[:required_pool_ratios]
#                     logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

#                 in_channels = sample.x.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 out_channels = sample.y.shape[1]
#                 num_heads = hyperparams.get('gtr_heads', 4)
#                 concat = hyperparams.get('gtr_concat', True)
#                 dropout = hyperparams.get('gtr_dropout', 0.0)
#                 edge_dim = sample.edge_attr.shape[1] if sample.edge_attr is not None else None

#                 model = GraphTransformer(
#                     in_channels=in_channels,
#                     hidden_dim=hidden_dim,
#                     out_channels=out_channels,
#                     num_layers=num_layers,
#                     pool_ratios=pool_ratios,
#                     num_heads=num_heads,
#                     concat=concat,
#                     dropout=dropout,
#                     edge_dim=edge_dim,
#                 )
#                 logging.info("Initialized GraphTransformer model.")

#             elif model_name == 'mgn':
#                 node_in_dim = sample.x.shape[1]
#                 edge_in_dim = sample.edge_attr.shape[1] if sample.edge_attr is not None else 0
#                 node_out_dim = sample.y.shape[1]
#                 hidden_dim = hyperparams['hidden_dim']
#                 num_layers = hyperparams['num_layers']

#                 model = MeshGraphNet(
#                     node_in_dim=node_in_dim,
#                     edge_in_dim=edge_in_dim,
#                     node_out_dim=node_out_dim,
#                     hidden_dim=hidden_dim,
#                     num_layers=num_layers
#                 )
#                 logging.info("Initialized MeshGraphNet model.")

#             else:
#                 logging.error(f"Unknown model '{model_name}'.")
#                 sys.exit(1)

#         return model


#     # Initialize the model
#     model = initialize_model(hyperparams, sample)
#     model.to(device)
#     logging.info(f"Model moved to {device}.")

#     # Load checkpoint
#     checkpoint = torch.load(args.checkpoint, map_location=device)
#     model.load_state_dict(checkpoint['model_state_dict'])
#     logging.info(f"Loaded model state dict from {args.checkpoint}")

#     # Create results folder
#     results_folder = args.results_folder
#     os.makedirs(results_folder, exist_ok=True)
#     logging.info(f"Results will be saved to {results_folder}")

#     # Path to metadata_final.json
#     metadata_final_path = os.path.join(final_graph_dir, 'metadata.json')
#     if not os.path.exists(metadata_final_path):
#         logging.error(f"metadata.json not found at {metadata_final_path}")
#         sys.exit(1)

#     # Evaluate model
#     evaluate_model(model, dataloader, device, metadata_final_path, results_folder)


# Filename: evaluate_checkpoint.py

import argparse
import os
import sys
import json
import logging
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torch.utils.data import Subset
from torch_geometric.loader import DataLoader  # Use PyTorch Geometric's DataLoader
from tqdm import tqdm
from pmd_beamphysics import ParticleGroup  # Import ParticleGroup

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Import your models and utilities
from datasets import GraphDataset
from utils import generate_data_dirs, set_random_seed
from src.graph_models.models.graph_networks import (
    GraphConvolutionNetwork,
    GraphAttentionNetwork,
    GraphTransformer,
    MeshGraphNet
)
from src.graph_models.models.graph_autoencoders import (
    GraphConvolutionalAutoEncoder,
    GraphAttentionAutoEncoder,
    GraphTransformerAutoEncoder,
    MeshGraphAutoEncoder
)
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.multiscale.gnn import (
    SinglescaleGNN,
    MultiscaleGNN,
    TopkMultiscaleGNN
)

def parse_hyperparameters_from_folder_name(folder_name):
    """
    Parses hyperparameters from the folder name and returns them as a dictionary.
    """
    hyperparams = {}
    parts = folder_name.split('_')
    known_prefixes = {
        'r', 'nt', 'b', 'lr', 'h', 'ly', 'pr', 'ep', 'sch',
        'heads', 'concat', 'dropout', 'mlph', 'mmply', 'mply'
    }
    idx = 0
    data_keyword_parts = []
    while idx < len(parts) and not any(parts[idx].startswith(prefix) for prefix in known_prefixes):
        data_keyword_parts.append(parts[idx])
        idx += 1
    hyperparams['data_keyword'] = '_'.join(data_keyword_parts)
    while idx < len(parts):
        part = parts[idx]
        matched = False
        for prefix in known_prefixes:
            if part.startswith(prefix):
                value = part[len(prefix):]
                if prefix == 'pr':  # pool_ratios
                    ratios = value.split('_')
                    hyperparams['pool_ratios'] = [float(r) for r in ratios]
                elif prefix == 'concat':
                    hyperparams['gtr_concat'] = value.lower() == 'true'
                elif prefix == 'sch':
                    hyperparams['lr_scheduler'] = value
                    if value.startswith('lin'):
                        lin_params = parts[idx + 1:idx + 4]
                        if len(lin_params) >= 3:
                            hyperparams['lin_start_epoch'] = int(lin_params[0])
                            hyperparams['lin_end_epoch'] = int(lin_params[1])
                            hyperparams['lin_final_lr'] = float(lin_params[2])
                            idx += 3  # Skip the parameters we just consumed
                else:
                    param_name = {
                        'r': 'random_seed',
                        'nt': 'ntrain',
                        'b': 'batch_size',
                        'lr': 'lr',
                        'h': 'hidden_dim',
                        'ly': 'num_layers',
                        'ep': 'nepochs',
                        'heads': 'heads',
                        'dropout': 'gtr_dropout',
                        'mlph': 'multiscale_n_mlp_hidden_layers',
                        'mmply': 'multiscale_n_mmp_layers',
                        'mply': 'multiscale_n_message_passing_layers',
                    }.get(prefix, prefix)
                    hyperparams[param_name] = type_cast(param_name, value)
                matched = True
                break
        if not matched:
            logging.warning(f"Unrecognized hyperparameter part: {part}")
        idx += 1
    return hyperparams

def extract_hyperparameters_from_checkpoint(checkpoint_path):
    """
    Extracts hyperparameters from the checkpoint path.
    """
    path_parts = [part for part in checkpoint_path.split(os.sep) if part]
    if 'checkpoints' not in path_parts:
        logging.error("'checkpoints' directory not found in the checkpoint path.")
        sys.exit(1)
    checkpoint_idx = path_parts.index('checkpoints')
    if checkpoint_idx < 4:
        logging.error("Checkpoint path is too short to extract hyperparameters.")
        sys.exit(1)
    folder_name = path_parts[checkpoint_idx - 1]
    task = path_parts[checkpoint_idx - 2]
    dataset = path_parts[checkpoint_idx - 3]
    model = path_parts[checkpoint_idx - 4]
    base_results_dir = os.sep + os.sep.join(path_parts[:checkpoint_idx - 4])
    hyperparams = parse_hyperparameters_from_folder_name(folder_name)
    hyperparams.update({
        'model': model,
        'dataset': dataset,
        'task': task,
        'base_results_dir': base_results_dir
    })
    return hyperparams

def type_cast(key, value):
    """
    Helper function to cast hyperparameter values to appropriate types.
    """
    int_params = {
        'random_seed', 'ntrain', 'batch_size', 'hidden_dim', 'num_layers', 'nepochs',
        'heads', 'multiscale_n_mlp_hidden_layers', 'multiscale_n_mmp_layers',
        'multiscale_n_message_passing_layers'
    }
    float_params = {'lr', 'gtr_dropout'}
    if key in int_params:
        return int(value)
    elif key in float_params:
        return float(value)
    else:
        return value

def check_missing_hyperparameters(hyperparams, required_params):
    missing_params = [param for param in required_params if param not in hyperparams]
    if missing_params:
        logging.error(f"Missing hyperparameters: {missing_params}")
        sys.exit(1)

def transform_to_particle_group(data):
    """
    Converts data tensor to ParticleGroup.

    Args:
        data (torch.Tensor): Tensor of shape [num_nodes, 6]

    Returns:
        ParticleGroup
    """
    num_particles = data.shape[0]
    particle_dict = {
        'x': data[:, 0].numpy(),
        'y': data[:, 1].numpy(),
        'z': data[:, 2].numpy(),
        'px': data[:, 3].numpy(),
        'py': data[:, 4].numpy(),
        'pz': data[:, 5].numpy(),
        'species': 'electron',
        'weight': np.full(num_particles, 2.e-17),
        't': np.zeros(num_particles),
        'status': np.ones(num_particles, dtype=int)
    }
    particle_group = ParticleGroup(data=particle_dict)
    return particle_group

def compute_normalized_emittance_x(particle_group):
    """
    Computes the normalized emittance in x direction for a ParticleGroup.

    Args:
        particle_group (ParticleGroup): The ParticleGroup for which to compute the emittance.

    Returns:
        float: The normalized emittance in x direction.
    """
    x = particle_group['x']
    px = particle_group['px']
    mean_x2 = np.mean(x**2)
    mean_px2 = np.mean(px**2)
    mean_xpx = np.mean(x * px)
    norm_emit_x = np.sqrt(mean_x2 * mean_px2 - mean_xpx**2)
    return norm_emit_x

def plot_particle_groups(pred_pg, target_pg, idx, error_type, results_folder):
    """
    Plots and saves figures for predicted and target ParticleGroups.

    Args:
        pred_pg (ParticleGroup): Predicted ParticleGroup
        target_pg (ParticleGroup): Target ParticleGroup
        idx (int): Sample index
        error_type (str): 'min' or 'max'
        results_folder (str): Folder to save figures
    """
    for x_var, p_var in [('x', 'px'), ('y', 'py'), ('z', 'pz')]:
        # Plot predicted
        plt.figure(figsize=(6, 6))
        pred_pg.plot(x_var, p_var, label='Predicted', alpha=0.6)
        plt.grid(True)
        plt.savefig(os.path.join(results_folder, f'{error_type}_mse_sample_{idx}_pred_{x_var}_{p_var}.png'))
        plt.close()
        # Plot target
        plt.figure(figsize=(6, 6))
        target_pg.plot(x_var, p_var, label='Target', alpha=0.6)
        plt.grid(True)
        plt.savefig(os.path.join(results_folder, f'{error_type}_mse_sample_{idx}_target_{x_var}_{p_var}.png'))
        plt.close()

def evaluate_model(model, dataloader, device, metadata_final_path, results_folder):
    model.eval()
    all_errors = []
    all_predictions = []
    all_targets = []
    with torch.no_grad():
        for data in tqdm(dataloader, desc="Evaluating Model"):
            data = data.to(device)
            x_pred = model_forward(model, data)
            mse = F.mse_loss(x_pred, data.y, reduction='none').mean(dim=1)
            batch_indices = data.batch.cpu().numpy()
            graph_indices = np.unique(batch_indices)
            for idx in graph_indices:
                mask = (batch_indices == idx)
                graph_mse = mse[mask].mean().item()
                all_errors.append(graph_mse)
                all_predictions.append(x_pred[mask].cpu())
                all_targets.append(data.y[mask].cpu())
    all_errors = np.array(all_errors)
    sorted_indices = np.argsort(all_errors)
    min_mse_indices = sorted_indices[:5]
    max_mse_indices = sorted_indices[-5:]

    global_mean, global_std, _, _ = load_global_statistics(metadata_final_path)

    def inverse_normalize(normalized_data):
        return normalized_data * global_std + global_mean

    # For min MSE samples
    for idx in min_mse_indices:
        pred = all_predictions[idx]
        target = all_targets[idx]
        pred_original = inverse_normalize(pred)
        target_original = inverse_normalize(target)
        pred_pg = transform_to_particle_group(pred_original)
        target_pg = transform_to_particle_group(target_original)
        plot_particle_groups(pred_pg, target_pg, idx, 'min', results_folder)
        pred_norm_emit_x = compute_normalized_emittance_x(pred_pg)
        target_norm_emit_x = compute_normalized_emittance_x(target_pg)
        relative_error = abs(pred_norm_emit_x - target_norm_emit_x) / abs(target_norm_emit_x)
        print(f"Sample {idx} (min MSE): Relative Error in norm_emit_x: {relative_error}")

    # For max MSE samples
    for idx in max_mse_indices:
        pred = all_predictions[idx]
        target = all_targets[idx]
        pred_original = inverse_normalize(pred)
        target_original = inverse_normalize(target)
        pred_pg = transform_to_particle_group(pred_original)
        target_pg = transform_to_particle_group(target_original)
        plot_particle_groups(pred_pg, target_pg, idx, 'max', results_folder)
        pred_norm_emit_x = compute_normalized_emittance_x(pred_pg)
        target_norm_emit_x = compute_normalized_emittance_x(target_pg)
        relative_error = abs(pred_norm_emit_x - target_norm_emit_x) / abs(target_norm_emit_x)
        print(f"Sample {idx} (max MSE): Relative Error in norm_emit_x: {relative_error}")

    # Compute overall test error
    test_error = all_errors.mean()
    print(f"Test Error (MSE): {test_error}")

def model_forward(model, data):
    """
    Forward pass for the model, handling different model types.
    """
    if isinstance(model, GNN_TopK):
        x_pred, _ = model(
            data.x,
            data.edge_index,
            data.edge_attr,
            data.pos,
            batch=data.batch
        )
    elif isinstance(model, TopkMultiscaleGNN):
        x_pred, _ = model(
            data.x,
            data.edge_index,
            data.pos,
            data.edge_attr,
            data.batch
        )
    elif isinstance(model, (SinglescaleGNN, MultiscaleGNN)):
        x_pred = model(
            data.x,
            data.edge_index,
            data.pos,
            data.edge_attr,
            data.batch
        )
    elif isinstance(model, (MeshGraphNet, MeshGraphAutoEncoder)):
        x_pred = model(
            data.x,
            data.edge_index,
            data.edge_attr,
            data.batch
        )
    elif isinstance(model, (GraphTransformer, GraphTransformerAutoEncoder)):
        x_pred = model(
            data.x,
            data.edge_index,
            data.edge_attr if hasattr(data, 'edge_attr') else None,
            data.batch
        )
    else:  # GraphConvolutionNetwork, GraphAttentionNetwork
        x_pred = model(
            data.x,
            data.edge_index,
            data.batch
        )
    return x_pred

def load_global_statistics(statistics_file):
    """Load global mean and standard deviation from a file."""
    with open(statistics_file, 'r') as f:
        lines = f.readlines()

    global_mean = []
    global_std = []
    settings_mean = None
    settings_std = None
    mode = None

    for line in lines:
        line = line.strip()
        if line == "Per-Step Global Mean:":
            mode = 'mean'
            continue
        elif line == "Per-Step Global Std:":
            mode = 'std'
            continue
        elif line.startswith("Settings Global Mean:"):
            settings_mean_str = line.split(":", 1)[1].strip()
            settings_mean = [float(x) for x in settings_mean_str.split(",")]
            mode = None
            continue
        elif line.startswith("Settings Global Std:"):
            settings_std_str = line.split(":", 1)[1].strip()
            settings_std = [float(x) for x in settings_std_str.split(",")]
            mode = None
            continue
        elif line.startswith("Step"):
            if mode in ('mean', 'std'):
                parts = line.split(":")
                if len(parts) != 2:
                    continue
                _, values_str = parts
                values = [float(x) for x in values_str.strip().split(",")]
                if mode == 'mean':
                    global_mean.append(values)
                elif mode == 'std':
                    global_std.append(values)
            else:
                continue
        else:
            continue

    # Assuming the last step contains the global statistics we need
    global_mean = torch.tensor(global_mean[-1], dtype=torch.float32)
    global_std = torch.tensor(global_std[-1], dtype=torch.float32)
    if settings_mean is not None:
        settings_mean = torch.tensor(settings_mean, dtype=torch.float32)
    if settings_std is not None:
        settings_std = torch.tensor(settings_std, dtype=torch.float32)

    return global_mean, global_std, settings_mean, settings_std

def main():
    parser = argparse.ArgumentParser(description="Evaluate a model checkpoint")
    parser.add_argument('--checkpoint', type=str, required=True, help="Path to the model checkpoint")
    parser.add_argument('--cpu_only', action='store_true', help="Force the script to use CPU even if GPU is available")
    parser.add_argument('--results_folder', type=str, default='evaluation_results', help="Folder to save evaluation results")
    parser.add_argument('--subsample_size', type=int, default=None, help="Number of samples to use for evaluation (if None, use all)")
    args = parser.parse_args()

    # Set device
    device = torch.device('cpu') if args.cpu_only else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # Extract hyperparameters from the checkpoint path
    hyperparams = extract_hyperparameters_from_checkpoint(args.checkpoint)
    logging.info(f"Extracted hyperparameters: {hyperparams}")

    # Define required hyperparameters
    required_hyperparams = [
        'model', 'dataset', 'task', 'data_keyword', 'random_seed', 'batch_size',
        'hidden_dim', 'num_layers', 'pool_ratios'
    ]

    # Check for missing hyperparameters
    check_missing_hyperparameters(hyperparams, required_params=required_hyperparams)

    # Set random seed
    set_random_seed(hyperparams['random_seed'])

    # Generate data directories
    initial_graph_dir, final_graph_dir, settings_dir = generate_data_dirs(
        hyperparams.get('base_data_dir', '/sdf/data/ad/ard/u/tiffan/data/'),
        hyperparams['dataset'],
        hyperparams['data_keyword']
    )
    logging.info(f"Initial graph directory: {initial_graph_dir}")
    logging.info(f"Final graph directory: {final_graph_dir}")
    logging.info(f"Settings directory: {settings_dir}")

    # Initialize dataset
    use_edge_attr = hyperparams['model'].lower() in [
        'intgnn', 'gtr', 'mgn', 'gtr-ae', 'mgn-ae',
        'singlescale', 'multiscale', 'multiscale-topk'
    ]
    logging.info(f"Model '{hyperparams['model']}' requires edge_attr: {use_edge_attr}")

    dataset = GraphDataset(
        initial_graph_dir=initial_graph_dir,
        final_graph_dir=final_graph_dir,
        settings_dir=settings_dir,
        task=hyperparams['task'],
        use_edge_attr=use_edge_attr
    )

    total_dataset_size = len(dataset)
    logging.info(f"Total dataset size: {total_dataset_size}")

    # Subset dataset if subsample_size is specified
    if args.subsample_size is not None:
        np.random.seed(hyperparams['random_seed'])  # For reproducibility
        if args.subsample_size >= total_dataset_size:
            logging.warning(f"Requested subsample_size {args.subsample_size} is greater than or equal to total dataset size {total_dataset_size}. Using full dataset.")
        else:
            indices = np.random.permutation(total_dataset_size)[:args.subsample_size]
            dataset = Subset(dataset, indices)
            logging.info(f"Using a subsample of {args.subsample_size} samples for evaluation.")

    dataloader = DataLoader(dataset, batch_size=hyperparams['batch_size'], shuffle=False)

    # Get a sample data for model initialization
    sample = dataset[0]

    # Initialize the model
    model = initialize_model(hyperparams, sample)
    model.to(device)
    logging.info(f"Model moved to {device}.")

    # Load checkpoint
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    logging.info(f"Loaded model state dict from {args.checkpoint}")

    # Create results folder
    results_folder = args.results_folder
    os.makedirs(results_folder, exist_ok=True)
    logging.info(f"Results will be saved to {results_folder}")

    # Path to metadata_final.json
    metadata_final_path = os.path.join(final_graph_dir, 'metadata.json')
    if not os.path.exists(metadata_final_path):
        logging.error(f"metadata.json not found at {metadata_final_path}")
        sys.exit(1)

    # Evaluate model
    evaluate_model(model, dataloader, device, metadata_final_path, results_folder)

if __name__ == "__main__":
    main()



# ========== trainers.py ==========

# trainers.py

import torch
import torch.optim as optim
from tqdm import tqdm
import logging
import matplotlib.pyplot as plt
from pathlib import Path
from src.graph_models.models.multiscale.gnn import SinglescaleGNN, MultiscaleGNN, TopkMultiscaleGNN
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.graph_networks import MeshGraphNet, GraphTransformer
from src.graph_models.models.graph_autoencoders import MeshGraphAutoEncoder, GraphTransformerAutoEncoder

class BaseTrainer:
    def __init__(self, model, dataloader, optimizer, scheduler=None, device='cpu', **kwargs):
        self.model = model.to(device)
        self.dataloader = dataloader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.start_epoch = 0
        self.nepochs = kwargs.get('nepochs', 100)
        self.save_checkpoint_every = kwargs.get('save_checkpoint_every', 10)
        self.results_folder = Path(kwargs.get('results_folder', './results'))
        self.results_folder.mkdir(parents=True, exist_ok=True)
        self.loss_history = []
        self.verbose = kwargs.get('verbose', False)

        # Create 'checkpoints' subfolder under results_folder
        self.checkpoints_folder = self.results_folder / 'checkpoints'
        self.checkpoints_folder.mkdir(parents=True, exist_ok=True)

        self.random_seed = kwargs.get('random_seed', 42)

        # Checkpoint
        self.checkpoint = kwargs.get('checkpoint', None)
        if self.checkpoint:
            self.load_checkpoint(self.checkpoint)

    def train(self):
        logging.info("Starting training...")
        for epoch in range(self.start_epoch, self.nepochs):
            self.model.train()
            total_loss = 0
            if self.verbose:
                progress_bar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.nepochs}")
            else:
                progress_bar = self.dataloader
            for data in progress_bar:
                data = data.to(self.device)
                self.optimizer.zero_grad()
                loss = self.train_step(data)
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
                if self.verbose:
                    progress_bar.set_postfix(loss=total_loss / len(self.dataloader))

            # Scheduler step
            if self.scheduler:
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                logging.info(f"Epoch {epoch+1}: Learning rate adjusted to {current_lr}")

            # Save loss history
            avg_loss = total_loss / len(self.dataloader)
            self.loss_history.append(avg_loss)
            logging.info(f'Epoch {epoch+1}/{self.nepochs}, Loss: {avg_loss:.4e}')

            # Save checkpoint
            if (epoch + 1) % self.save_checkpoint_every == 0 or (epoch + 1) == self.nepochs:
                self.save_checkpoint(epoch)

        # Plot loss convergence
        self.plot_loss_convergence()
        logging.info("Training complete!")

    def train_step(self, data):
        raise NotImplementedError("Subclasses should implement this method.")

    def save_checkpoint(self, epoch):
        checkpoint_path = self.checkpoints_folder / f'model-{epoch}.pth'
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'epoch': epoch + 1,  # Save the next epoch to resume from
            'random_seed': self.random_seed,
            'loss_history': self.loss_history,
        }, checkpoint_path)
        logging.info(f"Model checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path):
        logging.info(f"Loading checkpoint from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if self.scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.start_epoch = checkpoint['epoch']
        if 'random_seed' in checkpoint:
            self.random_seed = checkpoint['random_seed']
            logging.info(f"Using random seed from checkpoint: {self.random_seed}")
        if 'loss_history' in checkpoint:
            self.loss_history = checkpoint['loss_history']
        logging.info(f"Resumed training from epoch {self.start_epoch}")

    def plot_loss_convergence(self):
        plt.figure(figsize=(10, 6))
        plt.plot(self.loss_history, label="Training Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Loss Convergence")
        plt.legend()
        plt.grid(True)
        plt.savefig(self.results_folder / "loss_convergence.png")
        plt.close()


class GraphPredictionTrainer(BaseTrainer):        
    def __init__(self, criterion=None, **kwargs):
        super().__init__(**kwargs)
        
        # Set the loss function
        if criterion is not None:
            self.criterion = criterion
        else:
            # Default loss function for classification tasks
            self.criterion = torch.nn.MSELoss()
        
        logging.info(f"Using loss function: {self.criterion.__class__.__name__}")

    def train_step(self, data):
        x_pred = self.model_forward(data)
        loss = self.criterion(x_pred, data.y)
        return loss

    def model_forward(self, data):
        if isinstance(self.model, GNN_TopK):
            x_pred, _ = self.model(
                data.x,
                data.edge_index,
                data.edge_attr,
                data.pos,
                batch=data.batch
            )
        elif isinstance(self.model, TopkMultiscaleGNN):
            x_pred, mask = self.model(
                data.x,
                data.edge_index,
                data.pos,
                data.edge_attr,
                data.batch
            )
        elif isinstance(self.model, SinglescaleGNN) or isinstance(self.model, MultiscaleGNN):
            x_pred = self.model(
                data.x,
                data.edge_index,
                data.pos,
                data.edge_attr,
                data.batch
            )
        elif isinstance(self.model, MeshGraphNet) or isinstance(self.model, MeshGraphAutoEncoder):
            # MeshGraphNet uses edge attributes
            x_pred = self.model(
                data.x,
                data.edge_index,
                data.edge_attr,
                data.batch
            )
        elif isinstance(self.model, GraphTransformer) or isinstance(self.model, GraphTransformerAutoEncoder):
            x_pred = self.model(
                data.x,
                data.edge_index,
                data.edge_attr if hasattr(data, 'edge_attr') else None,
                data.batch
            )
        else: # GraphConvolutionNetwork, GraphAttentionNetwork
            x_pred = self.model(
                data.x,
                data.edge_index,
                data.batch
            )
        return x_pred


# ========== sequence_utils.py ==========

# sequence_utils.py

from datetime import datetime
import os
import numpy as np
import logging
import torch
import random

def generate_results_folder_name(args):
    # Base directory for results
    base_results_dir = args.base_results_dir

    # Incorporate model name
    base_results_dir = os.path.join(base_results_dir, args.model)
    
    # Incorporate dataset name
    base_results_dir = os.path.join(base_results_dir, args.dataset)

    # Modify task to include initial and final steps
    task_with_steps = f"{args.task}_init{args.initial_step}_final{args.final_step}"
    base_results_dir = os.path.join(base_results_dir, task_with_steps)

    # Extract important arguments
    parts = []
    parts.append(f"{args.data_keyword}")
    parts.append(f"r{args.random_seed}")
    parts.append(f"nt{args.ntrain if args.ntrain is not None else 'all'}")
    parts.append(f"b{args.batch_size}")
    parts.append(f"lr{args.lr}")
    parts.append(f"h{args.hidden_dim}")
    parts.append(f"ly{args.num_layers}")
    parts.append(f"pr{'_'.join(map(lambda x: f'{x:.2f}', args.pool_ratios))}")
    parts.append(f"ep{args.nepochs}")
    # Remove initial and final steps from parts since they're now in the task name
    # parts.append(f"init{args.initial_step}_final{args.final_step}")

    # Append scheduler info if used
    if args.lr_scheduler == 'exp':
        parts.append(f"sch_exp_{args.exp_decay_rate}_{args.exp_start_epoch}")
    elif args.lr_scheduler == 'lin':
        parts.append(f"sch_lin_{args.lin_start_epoch}_{args.lin_end_epoch}_{args.lin_final_lr}")

    # Model-specific arguments
    if args.model == 'gcn' or args.model == 'gcn-ae':
        pass
    elif args.model == 'gat' or args.model == 'gat-ae':
        parts.append(f"heads{args.gat_heads}")
    elif args.model == 'gtr' or args.model == 'gtr-ae':
        parts.append(f"heads{args.gtr_heads}")
        parts.append(f"concat{args.gtr_concat}")
        parts.append(f"dropout{args.gtr_dropout}")
    elif args.model == 'mgn' or args.model == 'mgn-ae':
        pass
    elif args.model == 'intgnn':
        pass  # Append any intgnn-specific parameters if needed
    elif args.model == 'singlescale':
        pass
    elif args.model == 'multiscale' or args.model == 'multiscale-topk':
        parts.append(f"mlph{args.multiscale_n_mlp_hidden_layers}")
        parts.append(f"mmply{args.multiscale_n_mmp_layers}")
        parts.append(f"mply{args.multiscale_n_message_passing_layers}")

    # Combine parts to form the folder name
    folder_name = '_'.join(map(str, parts))
    results_folder = os.path.join(base_results_dir, folder_name)
    return results_folder

def save_metadata(args, model, results_folder):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metadata_path = os.path.join(results_folder, f'metadata_{timestamp}.txt')
    os.makedirs(results_folder, exist_ok=True)
    with open(metadata_path, 'w') as f:
        f.write("=== Model Hyperparameters ===\n")
        hyperparams = vars(args)
        for key, value in hyperparams.items():
            if isinstance(value, list):
                value = ', '.join(map(str, value))
            f.write(f"{key}: {value}\n")

        f.write("\n=== Model Architecture ===\n")
        f.write(str(model))

    logging.info(f"Metadata saved to {metadata_path}")

def exponential_lr_scheduler(epoch, decay_rate=0.001, decay_start_epoch=0):
    if epoch < decay_start_epoch:
        return 1.0
    else:
        return np.exp(-decay_rate * (epoch - decay_start_epoch))

def linear_lr_scheduler(epoch, start_epoch=10, end_epoch=100, initial_lr=1e-4, final_lr=1e-6):
    if epoch < start_epoch:
        return 1.0
    elif start_epoch <= epoch < end_epoch:
        proportion = (epoch - start_epoch) / (end_epoch - start_epoch)
        lr = initial_lr + proportion * (final_lr - initial_lr)
        return lr / initial_lr
    else:
        return final_lr / initial_lr

def get_scheduler(args, optimizer):
    if args.lr_scheduler == 'exp':
        scheduler_func = lambda epoch: exponential_lr_scheduler(
            epoch,
            decay_rate=args.exp_decay_rate,
            decay_start_epoch=args.exp_start_epoch
        )
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_func)
    elif args.lr_scheduler == 'lin':
        scheduler_func = lambda epoch: linear_lr_scheduler(
            epoch,
            start_epoch=args.lin_start_epoch,
            end_epoch=args.lin_end_epoch,
            initial_lr=args.lr,
            final_lr=args.lin_final_lr
        )
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_func)
    else:
        scheduler = None
    return scheduler

def set_random_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ========== utils.py ==========

# utils.py

from datetime import datetime
import os
import numpy as np
import logging
import torch
import random

def generate_data_dirs(base_data_dir, dataset, data_keyword):
    initial_graph_dir = os.path.join(base_data_dir, f'{dataset}/initial_{data_keyword}_graphs')
    final_graph_dir = os.path.join(base_data_dir, f'{dataset}/final_{data_keyword}_graphs')
    settings_dir = os.path.join(base_data_dir, f'{dataset}/settings_{data_keyword}_graphs')
    return initial_graph_dir, final_graph_dir, settings_dir

def generate_results_folder_name(args):
    # Base directory for results
    base_results_dir = args.base_results_dir

    # Incorporate model name
    base_results_dir = os.path.join(base_results_dir, args.model)
    
    # Incorporate dataset name
    base_results_dir = os.path.join(base_results_dir, args.dataset)

    # Task-specific subfolder
    base_results_dir = os.path.join(base_results_dir, args.task)

    # Extract important arguments
    parts = []
    parts.append(f"{args.data_keyword}")
    parts.append(f"r{args.random_seed}")
    parts.append(f"nt{args.ntrain if args.ntrain is not None else 'all'}")
    parts.append(f"b{args.batch_size}")
    parts.append(f"lr{args.lr}")
    parts.append(f"h{args.hidden_dim}")
    parts.append(f"ly{args.num_layers}")
    parts.append(f"pr{'_'.join(map(lambda x: f'{x:.2f}', args.pool_ratios))}")
    parts.append(f"ep{args.nepochs}")

    # Append scheduler info if used
    if args.lr_scheduler == 'exp':
        parts.append(f"sch_exp_{args.exp_decay_rate}_{args.exp_start_epoch}")
    elif args.lr_scheduler == 'lin':
        parts.append(f"sch_lin_{args.lin_start_epoch}_{args.lin_end_epoch}_{args.lin_final_lr}")

    # Model-specific arguments
    if args.model == 'gcn' or args.model == 'gcn-ae':
        pass
    elif args.model == 'gat' or args.model == 'gat-ae':
        parts.append(f"heads{args.gat_heads}")
    elif args.model == 'gtr' or args.model == 'gtr-ae':
        parts.append(f"heads{args.gtr_heads}")
        parts.append(f"concat{args.gtr_concat}")
        parts.append(f"dropout{args.gtr_dropout}")
    elif args.model == 'mgn' or args.model == 'mgn-ae':
        pass
    elif args.model == 'intgnn':
        pass  # TODO: Append any intgnn-specific parameters
    elif args.model == 'singlescale':
        pass
    elif args.model == 'multiscale' or args.model == 'multiscale-topk':
        parts.append(f"mlph{args.multiscale_n_mlp_hidden_layers}")
        parts.append(f"mmply{args.multiscale_n_mmp_layers}")
        parts.append(f"mply{args.multiscale_n_message_passing_layers}")

    # Combine parts to form the folder name
    folder_name = '_'.join(map(str, parts))
    results_folder = os.path.join(base_results_dir, folder_name)
    return results_folder

def save_metadata(args, model, results_folder):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metadata_path = os.path.join(results_folder, f'metadata_{timestamp}.txt')
    os.makedirs(results_folder, exist_ok=True)
    with open(metadata_path, 'w') as f:
        f.write("=== Model Hyperparameters ===\n")
        hyperparams = vars(args)
        for key, value in hyperparams.items():
            if isinstance(value, list):
                value = ', '.join(map(str, value))
            f.write(f"{key}: {value}\n")

        f.write("\n=== Model Architecture ===\n")
        f.write(str(model))

    logging.info(f"Metadata saved to {metadata_path}")

def exponential_lr_scheduler(epoch, decay_rate=0.001, decay_start_epoch=0):
    if epoch < decay_start_epoch:
        return 1.0
    else:
        return np.exp(-decay_rate * (epoch - decay_start_epoch))

def linear_lr_scheduler(epoch, start_epoch=10, end_epoch=100, initial_lr=1e-4, final_lr=1e-6):
    if epoch < start_epoch:
        return 1.0
    elif start_epoch <= epoch < end_epoch:
        proportion = (epoch - start_epoch) / (end_epoch - start_epoch)
        lr = initial_lr + proportion * (final_lr - initial_lr)
        return lr / initial_lr
    else:
        return final_lr / initial_lr

def get_scheduler(args, optimizer):
    if args.lr_scheduler == 'exp':
        scheduler_func = lambda epoch: exponential_lr_scheduler(
            epoch,
            decay_rate=args.exp_decay_rate,
            decay_start_epoch=args.exp_start_epoch
        )
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_func)
    elif args.lr_scheduler == 'lin':
        scheduler_func = lambda epoch: linear_lr_scheduler(
            epoch,
            start_epoch=args.lin_start_epoch,
            end_epoch=args.lin_end_epoch,
            initial_lr=args.lr,
            final_lr=args.lin_final_lr
        )
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_func)
    else:
        scheduler = None
    return scheduler

def set_random_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ========== train_accelerate.py ==========

# train.py

import torch
import torch.nn.functional as F
from src.datasets.datasets import GraphDataset
from src.graph_models.models.graph_networks import (
    GraphConvolutionNetwork,
    GraphAttentionNetwork,
    GraphTransformer,
    MeshGraphNet
)
from src.graph_models.models.graph_autoencoders import (
    GraphConvolutionalAutoEncoder,
    GraphAttentionAutoEncoder,
    GraphTransformerAutoEncoder,
    MeshGraphAutoEncoder
)
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.multiscale.gnn import (
    SinglescaleGNN, 
    MultiscaleGNN, 
    TopkMultiscaleGNN
)

from trainers_accelerate import GraphPredictionTrainer
from utils import (
    generate_data_dirs,
    generate_results_folder_name,
    save_metadata,
    get_scheduler,
    set_random_seed
)
from config import parse_args
from torch.utils.data import Subset
from torch_geometric.loader import DataLoader
import numpy as np
import logging
import re
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def is_autoencoder_model(model_name):
    """
    Determines if the given model name corresponds to an autoencoder.

    Args:
        model_name (str): Name of the model.

    Returns:
        bool: True if it's an autoencoder model, False otherwise.
    """
    return model_name.lower().endswith('-ae') or model_name.lower() in ['multiscale-topk']

if __name__ == "__main__":
    args = parse_args()

    # Set device
    if args.cpu_only:
        device = torch.device('cpu')
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # Generate data directories
    initial_graph_dir, final_graph_dir, settings_dir = generate_data_dirs(
        args.base_data_dir, args.dataset, args.data_keyword
    )
    logging.info(f"Initial graph directory: {initial_graph_dir}")
    logging.info(f"Final graph directory: {final_graph_dir}")
    logging.info(f"Settings directory: {settings_dir}")

    # Generate results folder name
    if args.results_folder is not None:
        results_folder = args.results_folder
    else:
        results_folder = generate_results_folder_name(args)
    logging.info(f"Results will be saved to {results_folder}")

    # Set random seed
    set_random_seed(args.random_seed)

    # Determine if the model requires edge_attr
    # Removed 'gcn-ae' and 'gat-ae' as they do not require edge_attr for pooling
    models_requiring_edge_attr = ['intgnn', 'gtr', 'mgn', 'gtr-ae', 'mgn-ae', 'singlescale', 'multiscale', 'multiscale-topk'] # Adjusted list
    use_edge_attr = args.model.lower() in models_requiring_edge_attr
    logging.info(f"Model '{args.model}' requires edge_attr: {use_edge_attr}")

    # Initialize dataset
    dataset = GraphDataset(
        initial_graph_dir=initial_graph_dir,
        final_graph_dir=final_graph_dir,
        settings_dir=settings_dir,
        task=args.task,
        use_edge_attr=use_edge_attr
    )

    # Subset dataset if ntrain is specified
    total_dataset_size = len(dataset)
    if args.ntrain is not None:
        np.random.seed(args.random_seed)  # For reproducibility
        indices = np.random.permutation(total_dataset_size)[:args.ntrain]
        dataset = Subset(dataset, indices)
        
    # After initializing the dataset and subset
    for i in range(min(1, len(dataset))):
        sample = dataset[i]
        if hasattr(sample, 'edge_attr') and sample.edge_attr is not None:
            logging.info(f"Sample {i} edge_attr shape: {sample.edge_attr.shape}")
            # logging.info(f"Sample {i} edge_attr type: {type(sample.edge_attr)}")
            # logging.info(f"Sample {i} edge_attr dtype: {sample.edge_attr.dtype}")
        else:
            logging.warning(f"Sample {i} is missing edge_attr.")

    # Initialize dataloader
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    # Get a sample data for model initialization
    sample = dataset[0]

    # Model initialization
    if is_autoencoder_model(args.model):
        # Autoencoder models: 'gcn-ae', 'gat-ae', 'gtr-ae', 'mgn-ae'
        # Assert that args.num_layers is even
        if args.num_layers % 2 != 0:
            raise ValueError(f"For autoencoder models, 'num_layers' must be an even number. Received: {args.num_layers}")

        # Calculate depth as half of num_layers
        depth = args.num_layers // 2
        logging.info(f"Autoencoder selected. Using depth: {depth} (num_layers: {args.num_layers})")

        # Adjust pool_ratios to match depth - 1 (since pooling layers = depth -1)
        required_pool_ratios = depth - 1
        current_pool_ratios = len(args.pool_ratios)
        
        if required_pool_ratios <= 0:
            args.pool_ratios = []
            logging.info(f"No pooling layers required for depth {depth}.")
        elif current_pool_ratios < required_pool_ratios:
            # Pad pool_ratios with 1.0 to match required_pool_ratios
            args.pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
            logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
        elif current_pool_ratios > required_pool_ratios:
            # Trim pool_ratios to match required_pool_ratios
            args.pool_ratios = args.pool_ratios[:required_pool_ratios]
            logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

        # Initialize the corresponding autoencoder model
        if args.model.lower() == 'gcn-ae':
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]            
            pool_ratios = args.pool_ratios

            model = GraphConvolutionalAutoEncoder(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                depth=depth,
                pool_ratios=pool_ratios
            )
            logging.info("Initialized GraphConvolutionalAutoEncoder.")

        elif args.model.lower() == 'gat-ae':
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            pool_ratios = args.pool_ratios
            heads = args.gat_heads  # Ensure this argument exists

            model = GraphAttentionAutoEncoder(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                depth=depth,
                pool_ratios=pool_ratios,
                heads=heads
            )
            logging.info("Initialized GraphAttentionAutoEncoder.")

        elif args.model.lower() == 'gtr-ae':
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            pool_ratios = args.pool_ratios
            num_heads = args.gtr_heads  # Ensure this argument exists
            concat = args.gtr_concat    # Ensure this argument exists
            dropout = args.gtr_dropout  # Ensure this argument exists
            edge_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else None

            model = GraphTransformerAutoEncoder(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                depth=depth,
                pool_ratios=pool_ratios,
                num_heads=num_heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim
            )
            logging.info("Initialized GraphTransformerAutoEncoder.")

        elif args.model.lower() == 'mgn-ae':
            node_in_dim = sample.x.shape[1]
            edge_in_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            node_out_dim = sample.y.shape[1]  # Typically, autoencoder output matches input
            hidden_dim = args.hidden_dim
            pool_ratios = args.pool_ratios

            model = MeshGraphAutoEncoder(
                node_in_dim=node_in_dim,
                edge_in_dim=edge_in_dim,
                node_out_dim=node_out_dim,
                hidden_dim=hidden_dim,
                depth=depth,
                pool_ratios=pool_ratios
            )
            logging.info("Initialized MeshGraphAutoEncoder.")
            
        elif args.model.lower() == 'multiscale-topk':
            # Initialize TopkMultiscaleGNN
            input_node_channels = sample.x.shape[1]
            input_edge_channels = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            output_node_channels = sample.y.shape[1]
            n_mlp_hidden_layers = args.multiscale_n_mlp_hidden_layers
            n_mmp_layers = args.multiscale_n_mmp_layers
            n_messagePassing_layers = args.multiscale_n_message_passing_layers
            max_level_mmp = args.num_layers // 2 - 1  # n_levels = max_level + 1
            max_level_topk = args.num_layers // 2 - 1
            pool_ratios = args.pool_ratios

            # Compute l_char (characteristic length scale)
            edge_index = sample.edge_index
            pos = sample.pos
            edge_lengths = torch.norm(pos[edge_index[0]] - pos[edge_index[1]], dim=1)
            l_char = edge_lengths.mean().item()
            logging.info(f"Computed l_char (characteristic length scale): {l_char}")

            name = 'topk_multiscale_gnn'

            # Initialize model
            model = TopkMultiscaleGNN(
                input_node_channels=input_node_channels,
                input_edge_channels=input_edge_channels,
                hidden_channels=hidden_channels,
                output_node_channels=output_node_channels,
                n_mlp_hidden_layers=n_mlp_hidden_layers,
                n_mmp_layers=n_mmp_layers,
                n_messagePassing_layers=n_messagePassing_layers,
                max_level_mmp=max_level_mmp,
                max_level_topk=max_level_topk,
                # rf_topk=rf_topk,
                pool_ratios=pool_ratios,
                l_char=l_char,
                name=name
            )
            logging.info("Initialized TopkMultiscaleGNN model.")


        else:
            raise ValueError(f"Unknown autoencoder model {args.model}")

    else:
        # Non-autoencoder models: 'intgnn', 'gcn', 'gat', 'gtr', 'mgn'
        if args.model.lower() == 'intgnn':
            # Model parameters specific to GNN_TopK
            in_channels_node = sample.x.shape[1]
            in_channels_edge = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            out_channels = sample.y.shape[1]
            n_mlp_encode = 3
            n_mlp_mp = 2
            n_mp_down_topk = [1, 1]
            n_mp_up_topk = [1, 1]
            pool_ratios = args.pool_ratios
            n_mp_down_enc = [4]
            n_mp_up_enc = []
            lengthscales_enc = []
            n_mp_down_dec = [2, 2, 4]
            n_mp_up_dec = [2, 2]
            lengthscales_dec = [0.5, 1.0]
            interp = 'learned'
            act = F.elu
            param_sharing = False

            # Create bounding box if needed
            bounding_box = []
            if len(lengthscales_dec) > 0:
                x_lo = sample.pos[:, 0].min() - lengthscales_dec[0] / 2
                x_hi = sample.pos[:, 0].max() + lengthscales_dec[0] / 2
                y_lo = sample.pos[:, 1].min() - lengthscales_dec[0] / 2
                y_hi = sample.pos[:, 1].max() + lengthscales_dec[0] / 2
                z_lo = sample.pos[:, 2].min() - lengthscales_dec[0] / 2
                z_hi = sample.pos[:, 2].max() + lengthscales_dec[0] / 2
                bounding_box = [
                    x_lo.item(), x_hi.item(),
                    y_lo.item(), y_hi.item(),
                    z_lo.item(), z_hi.item()
                ]

            # Initialize GNN_TopK model
            model = GNN_TopK(
                in_channels_node,
                in_channels_edge,
                hidden_channels,
                out_channels,
                n_mlp_encode,
                n_mlp_mp,
                n_mp_down_topk,
                n_mp_up_topk,
                pool_ratios,
                n_mp_down_enc,
                n_mp_up_enc,
                n_mp_down_dec,
                n_mp_up_dec,
                lengthscales_enc,
                lengthscales_dec,
                bounding_box,
                interp,
                act,
                param_sharing,
                name='gnn_topk'
            )
            logging.info("Initialized GNN_TopK model.")
        
        elif args.model.lower() == 'singlescale':
            # Initialize SinglescaleGNN
            input_node_channels = sample.x.shape[1]
            input_edge_channels = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            output_node_channels = sample.y.shape[1]
            n_mlp_hidden_layers = 0  # Set based on MeshGraphNet
            n_messagePassing_layers = args.num_layers

            name = 'singlescale_gnn'

            # Initialize model
            model = SinglescaleGNN(
                input_node_channels=input_node_channels,
                input_edge_channels=input_edge_channels,
                hidden_channels=hidden_channels,
                output_node_channels=output_node_channels,
                n_mlp_hidden_layers=n_mlp_hidden_layers,
                n_messagePassing_layers=n_messagePassing_layers,
                name=name
            )
            logging.info("Initialized SinglescaleGNN model.")
            
        elif args.model.lower() == 'multiscale':
            # Initialize MultiscaleGNN
            input_node_channels = sample.x.shape[1]
            input_edge_channels = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            output_node_channels = sample.y.shape[1]
            n_mlp_hidden_layers = args.multiscale_n_mlp_hidden_layers
            n_mmp_layers = args.multiscale_n_mmp_layers
            n_messagePassing_layers = args.multiscale_n_message_passing_layers
            max_level = args.num_layers // 2 - 1  # n_levels = max_level + 1

            # Compute l_char (characteristic length scale)
            edge_index = sample.edge_index
            pos = sample.pos
            edge_lengths = torch.norm(pos[edge_index[0]] - pos[edge_index[1]], dim=1)
            l_char = edge_lengths.mean().item()
            logging.info(f"Computed l_char (characteristic length scale): {l_char}")

            name = 'multiscale_gnn'

            # Initialize model
            model = MultiscaleGNN(
                input_node_channels=input_node_channels,
                input_edge_channels=input_edge_channels,
                hidden_channels=hidden_channels,
                output_node_channels=output_node_channels,
                n_mlp_hidden_layers=n_mlp_hidden_layers,
                n_mmp_layers=n_mmp_layers,
                n_messagePassing_layers=n_messagePassing_layers,
                max_level=max_level,
                l_char=l_char,
                name=name
            )
            logging.info("Initialized MultiscaleGNN model.")

        elif args.model.lower() == 'gcn':
            # Ensure pool_ratios length matches num_layers - 2
            required_pool_ratios = args.num_layers - 2
            current_pool_ratios = len(args.pool_ratios)
            
            if required_pool_ratios <= 0:
                args.pool_ratios = []
                logging.info(f"No pooling layers required for num_layers {args.num_layers}.")
            elif current_pool_ratios < required_pool_ratios:
                args.pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
                logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
            elif current_pool_ratios > required_pool_ratios:
                args.pool_ratios = args.pool_ratios[:required_pool_ratios]
                logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

            # Model parameters specific to GraphConvolutionNetwork
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            num_layers = args.num_layers
            pool_ratios = args.pool_ratios

            # Initialize GraphConvolutionNetwork model
            model = GraphConvolutionNetwork(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                num_layers=num_layers,
                pool_ratios=pool_ratios,
            )
            logging.info("Initialized GraphConvolutionNetwork model.")

        elif args.model.lower() == 'gat':
            # Ensure pool_ratios length matches num_layers - 2 (since we don't pool after the last layer)
            required_pool_ratios = args.num_layers - 2
            current_pool_ratios = len(args.pool_ratios)
            
            if required_pool_ratios <= 0:
                args.pool_ratios = []
                logging.info(f"No pooling layers required for num_layers {args.num_layers}.")
            elif current_pool_ratios < required_pool_ratios:
                # Pad pool_ratios with 1.0 to match required_pool_ratios
                args.pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
                logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
            elif current_pool_ratios > required_pool_ratios:
                # Trim pool_ratios to match required_pool_ratios
                args.pool_ratios = args.pool_ratios[:required_pool_ratios]
                logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

            # Model parameters specific to GraphAttentionNetwork
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            num_layers = args.num_layers
            pool_ratios = args.pool_ratios
            heads = args.gat_heads  # Ensure this argument exists

            # Initialize GraphAttentionNetwork model
            model = GraphAttentionNetwork(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                num_layers=num_layers,
                pool_ratios=pool_ratios,
                heads=heads,
            )
            logging.info("Initialized GraphAttentionNetwork model.")

        elif args.model.lower() == 'gtr':
            # Model parameters specific to GraphTransformer
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            num_layers = args.num_layers
            pool_ratios = args.pool_ratios
            num_heads = args.gtr_heads
            concat = args.gtr_concat
            dropout = args.gtr_dropout
            edge_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else None

            # Adjust pool_ratios to match num_layers - 2
            required_pool_ratios = num_layers - 2
            current_pool_ratios = len(pool_ratios)
            
            if required_pool_ratios <= 0:
                pool_ratios = []
                logging.info(f"No pooling layers required for num_layers {num_layers}.")
            elif current_pool_ratios < required_pool_ratios:
                pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
                logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
            elif current_pool_ratios > required_pool_ratios:
                pool_ratios = pool_ratios[:required_pool_ratios]
                logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

            # Initialize GraphTransformer model
            model = GraphTransformer(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                num_layers=num_layers,
                pool_ratios=pool_ratios,
                num_heads=num_heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim,
            )
            logging.info("Initialized GraphTransformer model.")

        elif args.model.lower() == 'mgn':
            # Model parameters specific to MeshGraphNet
            node_in_dim = sample.x.shape[1]
            edge_in_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            node_out_dim = sample.y.shape[1]
            hidden_dim = args.hidden_dim
            num_layers = args.num_layers

            # Initialize MeshGraphNet model
            model = MeshGraphNet(
                node_in_dim=node_in_dim,
                edge_in_dim=edge_in_dim,
                node_out_dim=node_out_dim,
                hidden_dim=hidden_dim,
                num_layers=num_layers
            )
            logging.info("Initialized MeshGraphNet model.")

        else:
            raise ValueError(f"Unknown model {args.model}")

    model.to(device)
    logging.info(f"Model moved to {device}.")

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    logging.info(f"Initialized Adam optimizer with learning rate: {args.lr}")

    # Scheduler
    scheduler = get_scheduler(args, optimizer)
    if scheduler:
        logging.info("Initialized learning rate scheduler.")

    # Handle checkpoint
    if args.checkpoint is None and args.checkpoint_epoch is not None:
        results_folder_ = re.sub(r'ep\d+', f'ep{args.checkpoint_epoch}', results_folder)
        checkpoint_path = os.path.join(results_folder_, 'checkpoints', f'model-{args.checkpoint_epoch - 1}.pth')
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint for epoch {args.checkpoint_epoch} not found at {checkpoint_path}. Exiting.")
            exit(1)
        else:
            args.checkpoint = checkpoint_path
            logging.info(f"Checkpoint set to: {args.checkpoint}")

    if args.checkpoint is not None and args.checkpoint_epoch is None:
        checkpoint_path = args.checkpoint
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint not found at {checkpoint_path}. Exiting.")
            exit(1)  
        else:
            logging.info(f"Checkpoint set to: {checkpoint_path}")

    # Define the loss function based solely on node feature reconstruction
    criterion = torch.nn.MSELoss()

    # Initialize trainer with the custom loss function
    trainer = GraphPredictionTrainer(
        model=model,
        dataloader=dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        nepochs=args.nepochs,
        save_checkpoint_every=args.save_checkpoint_every,
        results_folder=results_folder,
        checkpoint=args.checkpoint,
        random_seed=args.random_seed,
        device=device,
        verbose=args.verbose,
        criterion=criterion  # Pass the custom loss function here
    )
    logging.info("Initialized GraphPredictionTrainer with custom loss function.")
    
    # Save metadata
    save_metadata(args, model, results_folder)

    # Run train or evaluate
    if args.mode == 'train':
        trainer.train()
    else:
        # Implement evaluation if needed
        logging.info("Evaluation mode is not implemented yet.")
        pass


# ========== test_sequence.sh ==========

python src/graph_models/sequence_train.py \
    --model gcn \
    --dataset sequence_graph_data_archive_4\
    --data_keyword knn_edges_k5_weighted \
    --task predict_n6d \
    --initial_step 0 \
    --final_step 1 \
    --identical_settings \
    --settings_file /sdf/data/ad/ard/u/tiffan/data/sequence_particles_data_archive_4/settings.pt \
    --ntrain 100 \
    --nepochs 10 \
    --lr 0.001 \
    --batch_size 8 \
    --hidden_dim 64 \
    --num_layers 3 \
    --pool_ratios 1.0 \
    --verbose


# ========== train.py ==========

# train.py

import torch
import torch.nn.functional as F
from src.datasets.datasets import GraphDataset
from src.graph_models.models.graph_networks import (
    GraphConvolutionNetwork,
    GraphAttentionNetwork,
    GraphTransformer,
    MeshGraphNet
)
from src.graph_models.models.graph_autoencoders import (
    GraphConvolutionalAutoEncoder,
    GraphAttentionAutoEncoder,
    GraphTransformerAutoEncoder,
    MeshGraphAutoEncoder
)
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.multiscale.gnn import (
    SinglescaleGNN, 
    MultiscaleGNN, 
    TopkMultiscaleGNN
)

from trainers import GraphPredictionTrainer
from utils import (
    generate_data_dirs,
    generate_results_folder_name,
    save_metadata,
    get_scheduler,
    set_random_seed
)
from config import parse_args
from torch.utils.data import Subset
from torch_geometric.loader import DataLoader
import numpy as np
import logging
import re
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def is_autoencoder_model(model_name):
    """
    Determines if the given model name corresponds to an autoencoder.

    Args:
        model_name (str): Name of the model.

    Returns:
        bool: True if it's an autoencoder model, False otherwise.
    """
    return model_name.lower().endswith('-ae') or model_name.lower() in ['multiscale-topk']

if __name__ == "__main__":
    args = parse_args()

    # Set device
    if args.cpu_only:
        device = torch.device('cpu')
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # Generate data directories
    initial_graph_dir, final_graph_dir, settings_dir = generate_data_dirs(
        args.base_data_dir, args.dataset, args.data_keyword
    )
    logging.info(f"Initial graph directory: {initial_graph_dir}")
    logging.info(f"Final graph directory: {final_graph_dir}")
    logging.info(f"Settings directory: {settings_dir}")

    # Generate results folder name
    if args.results_folder is not None:
        results_folder = args.results_folder
    else:
        results_folder = generate_results_folder_name(args)
    logging.info(f"Results will be saved to {results_folder}")

    # Set random seed
    set_random_seed(args.random_seed)

    # Determine if the model requires edge_attr
    # Removed 'gcn-ae' and 'gat-ae' as they do not require edge_attr for pooling
    models_requiring_edge_attr = ['intgnn', 'gtr', 'mgn', 'gtr-ae', 'mgn-ae', 'singlescale', 'multiscale', 'multiscale-topk'] # Adjusted list
    use_edge_attr = args.model.lower() in models_requiring_edge_attr
    logging.info(f"Model '{args.model}' requires edge_attr: {use_edge_attr}")

    # Initialize dataset
    dataset = GraphDataset(
        initial_graph_dir=initial_graph_dir,
        final_graph_dir=final_graph_dir,
        settings_dir=settings_dir,
        task=args.task,
        use_edge_attr=use_edge_attr
    )

    # Subset dataset if ntrain is specified
    total_dataset_size = len(dataset)
    if args.ntrain is not None:
        np.random.seed(args.random_seed)  # For reproducibility
        indices = np.random.permutation(total_dataset_size)[:args.ntrain]
        dataset = Subset(dataset, indices)
        
    # After initializing the dataset and subset
    for i in range(min(1, len(dataset))):
        sample = dataset[i]
        if hasattr(sample, 'edge_attr') and sample.edge_attr is not None:
            logging.info(f"Sample {i} edge_attr shape: {sample.edge_attr.shape}")
            # logging.info(f"Sample {i} edge_attr type: {type(sample.edge_attr)}")
            # logging.info(f"Sample {i} edge_attr dtype: {sample.edge_attr.dtype}")
        else:
            logging.warning(f"Sample {i} is missing edge_attr.")

    # Initialize dataloader
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    # Get a sample data for model initialization
    sample = dataset[0]

    # Model initialization
    if is_autoencoder_model(args.model):
        # Autoencoder models: 'gcn-ae', 'gat-ae', 'gtr-ae', 'mgn-ae'
        # Assert that args.num_layers is even
        if args.num_layers % 2 != 0:
            raise ValueError(f"For autoencoder models, 'num_layers' must be an even number. Received: {args.num_layers}")

        # Calculate depth as half of num_layers
        depth = args.num_layers // 2
        logging.info(f"Autoencoder selected. Using depth: {depth} (num_layers: {args.num_layers})")

        # Adjust pool_ratios to match depth - 1 (since pooling layers = depth -1)
        required_pool_ratios = depth - 1
        current_pool_ratios = len(args.pool_ratios)
        
        if required_pool_ratios <= 0:
            args.pool_ratios = []
            logging.info(f"No pooling layers required for depth {depth}.")
        elif current_pool_ratios < required_pool_ratios:
            # Pad pool_ratios with 1.0 to match required_pool_ratios
            args.pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
            logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
        elif current_pool_ratios > required_pool_ratios:
            # Trim pool_ratios to match required_pool_ratios
            args.pool_ratios = args.pool_ratios[:required_pool_ratios]
            logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

        # Initialize the corresponding autoencoder model
        if args.model.lower() == 'gcn-ae':
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]            
            pool_ratios = args.pool_ratios

            model = GraphConvolutionalAutoEncoder(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                depth=depth,
                pool_ratios=pool_ratios
            )
            logging.info("Initialized GraphConvolutionalAutoEncoder.")

        elif args.model.lower() == 'gat-ae':
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            pool_ratios = args.pool_ratios
            heads = args.gat_heads  # Ensure this argument exists

            model = GraphAttentionAutoEncoder(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                depth=depth,
                pool_ratios=pool_ratios,
                heads=heads
            )
            logging.info("Initialized GraphAttentionAutoEncoder.")

        elif args.model.lower() == 'gtr-ae':
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            pool_ratios = args.pool_ratios
            num_heads = args.gtr_heads  # Ensure this argument exists
            concat = args.gtr_concat    # Ensure this argument exists
            dropout = args.gtr_dropout  # Ensure this argument exists
            edge_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else None

            model = GraphTransformerAutoEncoder(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                depth=depth,
                pool_ratios=pool_ratios,
                num_heads=num_heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim
            )
            logging.info("Initialized GraphTransformerAutoEncoder.")

        elif args.model.lower() == 'mgn-ae':
            node_in_dim = sample.x.shape[1]
            edge_in_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            node_out_dim = sample.y.shape[1]  # Typically, autoencoder output matches input
            hidden_dim = args.hidden_dim
            pool_ratios = args.pool_ratios

            model = MeshGraphAutoEncoder(
                node_in_dim=node_in_dim,
                edge_in_dim=edge_in_dim,
                node_out_dim=node_out_dim,
                hidden_dim=hidden_dim,
                depth=depth,
                pool_ratios=pool_ratios
            )
            logging.info("Initialized MeshGraphAutoEncoder.")
            
        elif args.model.lower() == 'multiscale-topk':
            # Initialize TopkMultiscaleGNN
            input_node_channels = sample.x.shape[1]
            input_edge_channels = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            output_node_channels = sample.y.shape[1]
            n_mlp_hidden_layers = args.multiscale_n_mlp_hidden_layers
            n_mmp_layers = args.multiscale_n_mmp_layers
            n_messagePassing_layers = args.multiscale_n_message_passing_layers
            max_level_mmp = args.num_layers // 2 - 1  # n_levels = max_level + 1
            max_level_topk = args.num_layers // 2 - 1
            pool_ratios = args.pool_ratios

            # Compute l_char (characteristic length scale)
            edge_index = sample.edge_index
            pos = sample.pos
            edge_lengths = torch.norm(pos[edge_index[0]] - pos[edge_index[1]], dim=1)
            l_char = edge_lengths.mean().item()
            logging.info(f"Computed l_char (characteristic length scale): {l_char}")

            name = 'topk_multiscale_gnn'

            # Initialize model
            model = TopkMultiscaleGNN(
                input_node_channels=input_node_channels,
                input_edge_channels=input_edge_channels,
                hidden_channels=hidden_channels,
                output_node_channels=output_node_channels,
                n_mlp_hidden_layers=n_mlp_hidden_layers,
                n_mmp_layers=n_mmp_layers,
                n_messagePassing_layers=n_messagePassing_layers,
                max_level_mmp=max_level_mmp,
                max_level_topk=max_level_topk,
                # rf_topk=rf_topk,
                pool_ratios=pool_ratios,
                l_char=l_char,
                name=name
            )
            logging.info("Initialized TopkMultiscaleGNN model.")


        else:
            raise ValueError(f"Unknown autoencoder model {args.model}")

    else:
        # Non-autoencoder models: 'intgnn', 'gcn', 'gat', 'gtr', 'mgn'
        if args.model.lower() == 'intgnn':
            # Model parameters specific to GNN_TopK
            in_channels_node = sample.x.shape[1]
            in_channels_edge = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            out_channels = sample.y.shape[1]
            n_mlp_encode = 3
            n_mlp_mp = 2
            n_mp_down_topk = [1, 1]
            n_mp_up_topk = [1, 1]
            pool_ratios = args.pool_ratios
            n_mp_down_enc = [4]
            n_mp_up_enc = []
            lengthscales_enc = []
            n_mp_down_dec = [2, 2, 4]
            n_mp_up_dec = [2, 2]
            lengthscales_dec = [0.5, 1.0]
            interp = 'learned'
            act = F.elu
            param_sharing = False

            # Create bounding box if needed
            bounding_box = []
            if len(lengthscales_dec) > 0:
                x_lo = sample.pos[:, 0].min() - lengthscales_dec[0] / 2
                x_hi = sample.pos[:, 0].max() + lengthscales_dec[0] / 2
                y_lo = sample.pos[:, 1].min() - lengthscales_dec[0] / 2
                y_hi = sample.pos[:, 1].max() + lengthscales_dec[0] / 2
                z_lo = sample.pos[:, 2].min() - lengthscales_dec[0] / 2
                z_hi = sample.pos[:, 2].max() + lengthscales_dec[0] / 2
                bounding_box = [
                    x_lo.item(), x_hi.item(),
                    y_lo.item(), y_hi.item(),
                    z_lo.item(), z_hi.item()
                ]

            # Initialize GNN_TopK model
            model = GNN_TopK(
                in_channels_node,
                in_channels_edge,
                hidden_channels,
                out_channels,
                n_mlp_encode,
                n_mlp_mp,
                n_mp_down_topk,
                n_mp_up_topk,
                pool_ratios,
                n_mp_down_enc,
                n_mp_up_enc,
                n_mp_down_dec,
                n_mp_up_dec,
                lengthscales_enc,
                lengthscales_dec,
                bounding_box,
                interp,
                act,
                param_sharing,
                name='gnn_topk'
            )
            logging.info("Initialized GNN_TopK model.")
        
        elif args.model.lower() == 'singlescale':
            # Initialize SinglescaleGNN
            input_node_channels = sample.x.shape[1]
            input_edge_channels = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            output_node_channels = sample.y.shape[1]
            n_mlp_hidden_layers = 0  # Set based on MeshGraphNet
            n_messagePassing_layers = args.num_layers

            name = 'singlescale_gnn'

            # Initialize model
            model = SinglescaleGNN(
                input_node_channels=input_node_channels,
                input_edge_channels=input_edge_channels,
                hidden_channels=hidden_channels,
                output_node_channels=output_node_channels,
                n_mlp_hidden_layers=n_mlp_hidden_layers,
                n_messagePassing_layers=n_messagePassing_layers,
                name=name
            )
            logging.info("Initialized SinglescaleGNN model.")
            
        elif args.model.lower() == 'multiscale':
            # Initialize MultiscaleGNN
            input_node_channels = sample.x.shape[1]
            input_edge_channels = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            hidden_channels = args.hidden_dim
            output_node_channels = sample.y.shape[1]
            n_mlp_hidden_layers = args.multiscale_n_mlp_hidden_layers
            n_mmp_layers = args.multiscale_n_mmp_layers
            n_messagePassing_layers = args.multiscale_n_message_passing_layers
            max_level = args.num_layers // 2 - 1  # n_levels = max_level + 1

            # Compute l_char (characteristic length scale)
            edge_index = sample.edge_index
            pos = sample.pos
            edge_lengths = torch.norm(pos[edge_index[0]] - pos[edge_index[1]], dim=1)
            l_char = edge_lengths.mean().item()
            logging.info(f"Computed l_char (characteristic length scale): {l_char}")

            name = 'multiscale_gnn'

            # Initialize model
            model = MultiscaleGNN(
                input_node_channels=input_node_channels,
                input_edge_channels=input_edge_channels,
                hidden_channels=hidden_channels,
                output_node_channels=output_node_channels,
                n_mlp_hidden_layers=n_mlp_hidden_layers,
                n_mmp_layers=n_mmp_layers,
                n_messagePassing_layers=n_messagePassing_layers,
                max_level=max_level,
                l_char=l_char,
                name=name
            )
            logging.info("Initialized MultiscaleGNN model.")

        elif args.model.lower() == 'gcn':
            # Ensure pool_ratios length matches num_layers - 2
            required_pool_ratios = args.num_layers - 2
            current_pool_ratios = len(args.pool_ratios)
            
            if required_pool_ratios <= 0:
                args.pool_ratios = []
                logging.info(f"No pooling layers required for num_layers {args.num_layers}.")
            elif current_pool_ratios < required_pool_ratios:
                args.pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
                logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
            elif current_pool_ratios > required_pool_ratios:
                args.pool_ratios = args.pool_ratios[:required_pool_ratios]
                logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

            # Model parameters specific to GraphConvolutionNetwork
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            num_layers = args.num_layers
            pool_ratios = args.pool_ratios

            # Initialize GraphConvolutionNetwork model
            model = GraphConvolutionNetwork(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                num_layers=num_layers,
                pool_ratios=pool_ratios,
            )
            logging.info("Initialized GraphConvolutionNetwork model.")

        elif args.model.lower() == 'gat':
            # Ensure pool_ratios length matches num_layers - 2 (since we don't pool after the last layer)
            required_pool_ratios = args.num_layers - 2
            current_pool_ratios = len(args.pool_ratios)
            
            if required_pool_ratios <= 0:
                args.pool_ratios = []
                logging.info(f"No pooling layers required for num_layers {args.num_layers}.")
            elif current_pool_ratios < required_pool_ratios:
                # Pad pool_ratios with 1.0 to match required_pool_ratios
                args.pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
                logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
            elif current_pool_ratios > required_pool_ratios:
                # Trim pool_ratios to match required_pool_ratios
                args.pool_ratios = args.pool_ratios[:required_pool_ratios]
                logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

            # Model parameters specific to GraphAttentionNetwork
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            num_layers = args.num_layers
            pool_ratios = args.pool_ratios
            heads = args.gat_heads  # Ensure this argument exists

            # Initialize GraphAttentionNetwork model
            model = GraphAttentionNetwork(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                num_layers=num_layers,
                pool_ratios=pool_ratios,
                heads=heads,
            )
            logging.info("Initialized GraphAttentionNetwork model.")

        elif args.model.lower() == 'gtr':
            # Model parameters specific to GraphTransformer
            in_channels = sample.x.shape[1]
            hidden_dim = args.hidden_dim
            out_channels = sample.y.shape[1]
            num_layers = args.num_layers
            pool_ratios = args.pool_ratios
            num_heads = args.gtr_heads
            concat = args.gtr_concat
            dropout = args.gtr_dropout
            edge_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else None

            # Adjust pool_ratios to match num_layers - 2
            required_pool_ratios = num_layers - 2
            current_pool_ratios = len(pool_ratios)
            
            if required_pool_ratios <= 0:
                pool_ratios = []
                logging.info(f"No pooling layers required for num_layers {num_layers}.")
            elif current_pool_ratios < required_pool_ratios:
                pool_ratios += [1.0] * (required_pool_ratios - current_pool_ratios)
                logging.warning(f"Pool ratios were padded with 1.0 to match required_pool_ratios: {required_pool_ratios}")
            elif current_pool_ratios > required_pool_ratios:
                pool_ratios = pool_ratios[:required_pool_ratios]
                logging.warning(f"Pool ratios were trimmed to match required_pool_ratios: {required_pool_ratios}")

            # Initialize GraphTransformer model
            model = GraphTransformer(
                in_channels=in_channels,
                hidden_dim=hidden_dim,
                out_channels=out_channels,
                num_layers=num_layers,
                pool_ratios=pool_ratios,
                num_heads=num_heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim,
            )
            logging.info("Initialized GraphTransformer model.")

        elif args.model.lower() == 'mgn':
            # Model parameters specific to MeshGraphNet
            node_in_dim = sample.x.shape[1]
            edge_in_dim = sample.edge_attr.shape[1] if hasattr(sample, 'edge_attr') and sample.edge_attr is not None else 0
            node_out_dim = sample.y.shape[1]
            hidden_dim = args.hidden_dim
            num_layers = args.num_layers

            # Initialize MeshGraphNet model
            model = MeshGraphNet(
                node_in_dim=node_in_dim,
                edge_in_dim=edge_in_dim,
                node_out_dim=node_out_dim,
                hidden_dim=hidden_dim,
                num_layers=num_layers
            )
            logging.info("Initialized MeshGraphNet model.")

        else:
            raise ValueError(f"Unknown model {args.model}")

    model.to(device)
    logging.info(f"Model moved to {device}.")

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    logging.info(f"Initialized Adam optimizer with learning rate: {args.lr}")

    # Scheduler
    scheduler = get_scheduler(args, optimizer)
    if scheduler:
        logging.info("Initialized learning rate scheduler.")

    # Handle checkpoint
    if args.checkpoint is None and args.checkpoint_epoch is not None:
        results_folder_ = re.sub(r'ep\d+', f'ep{args.checkpoint_epoch}', results_folder)
        checkpoint_path = os.path.join(results_folder_, 'checkpoints', f'model-{args.checkpoint_epoch - 1}.pth')
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint for epoch {args.checkpoint_epoch} not found at {checkpoint_path}. Exiting.")
            exit(1)
        else:
            args.checkpoint = checkpoint_path
            logging.info(f"Checkpoint set to: {args.checkpoint}")

    if args.checkpoint is not None and args.checkpoint_epoch is None:
        checkpoint_path = args.checkpoint
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint not found at {checkpoint_path}. Exiting.")
            exit(1)  
        else:
            logging.info(f"Checkpoint set to: {checkpoint_path}")

    # Define the loss function based solely on node feature reconstruction
    criterion = torch.nn.MSELoss()

    # Initialize trainer with the custom loss function
    trainer = GraphPredictionTrainer(
        model=model,
        dataloader=dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        nepochs=args.nepochs,
        save_checkpoint_every=args.save_checkpoint_every,
        results_folder=results_folder,
        checkpoint=args.checkpoint,
        random_seed=args.random_seed,
        device=device,
        verbose=args.verbose,
        criterion=criterion  # Pass the custom loss function here
    )
    logging.info("Initialized GraphPredictionTrainer with custom loss function.")
    
    # Save metadata
    save_metadata(args, model, results_folder)

    # Run train or evaluate
    if args.mode == 'train':
        trainer.train()
    else:
        # Implement evaluation if needed
        logging.info("Evaluation mode is not implemented yet.")
        pass


# ========== config.py ==========

# config.py

import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="Train and evaluate graph-based models.")

    # Common arguments
    parser.add_argument('--model', type=str, required=True, choices=['intgnn', 'gcn', 'gat', 'gtr', 'mgn', 'gcn-ae', 'gat-ae', 'gtr-ae', 'mgn-ae', 'multiscale', 'multiscale-topk'], help="Model to train.")
    parser.add_argument('--data_keyword', type=str, required=True, help="Common keyword to infer data directories")
    parser.add_argument('--base_data_dir', type=str, default="/sdf/data/ad/ard/u/tiffan/data/",
                        help="Base directory where the data is stored")
    parser.add_argument('--base_results_dir', type=str, default="/sdf/data/ad/ard/u/tiffan/results/",
                        help="Base directory where the results are stored.")

    parser.add_argument('--dataset', type=str, required=True, help="Dataset identifier")
    parser.add_argument('--task', type=str, required=True, choices=['predict_n6d', 'predict_n4d', 'predict_n2d'],
                        help="Task to perform")
    parser.add_argument('--ntrain', type=int, default=None, help="Number of training examples to use")
    parser.add_argument('--nepochs', type=int, default=100, help="Number of training epochs")
    parser.add_argument('--save_checkpoint_every', type=int, default=10, help="Save checkpoint every N epochs")
    parser.add_argument('--lr', type=float, default=1e-4, help="Learning rate for training")
    parser.add_argument('--batch_size', type=int, default=8, help="Batch size for training")
    parser.add_argument('--hidden_dim', type=int, default=64, help="Hidden layer dimension size")
    parser.add_argument('--num_layers', type=int, default=3, help="Number of layers in the model")
    parser.add_argument('--pool_ratios', type=float, nargs='+', default=[1.0], help="Pooling ratios for TopKPooling layers")
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'evaluate'], help="Mode to run")
    parser.add_argument('--checkpoint', type=str, default=None, help="Path to a checkpoint to resume training")
    parser.add_argument('--checkpoint_epoch', type=int, default=None, help="Epoch of the checkpoint to load")
    parser.add_argument('--results_folder', type=str, default=None, help="Directory to save results and checkpoints")
    parser.add_argument('--random_seed', type=int, default=63, help="Random seed for reproducibility")
    parser.add_argument('--cpu_only', action='store_true', help="Force the script to use CPU even if GPU is available")
    parser.add_argument('--verbose', action='store_true', help="Display progress bar while training")

    # Learning Rate Scheduler Arguments
    parser.add_argument('--lr_scheduler', type=str, choices=['none', 'exp', 'lin'], default='none',
                        help="Learning rate scheduler type: 'none', 'exp', or 'lin'")
    # Exponential Scheduler Parameters
    parser.add_argument('--exp_decay_rate', type=float, default=0.001, help="Decay rate for exponential scheduler")
    parser.add_argument('--exp_start_epoch', type=int, default=0, help="Start epoch for exponential scheduler")
    # Linear Scheduler Parameters
    parser.add_argument('--lin_start_epoch', type=int, default=100, help="Start epoch for linear scheduler")
    parser.add_argument('--lin_end_epoch', type=int, default=1000, help="End epoch for linear scheduler")
    parser.add_argument('--lin_final_lr', type=float, default=1e-5, help="Final learning rate for linear scheduler")
    
    # GAT-specific arguments
    parser.add_argument('--gat_heads', type=int, default=1, help="Number of attention heads for GAT layers")

    # Graph Transformer-specific arguments
    parser.add_argument('--gtr_heads', type=int, default=4, help="Number of attention heads for TransformerConv layers")
    parser.add_argument('--gtr_concat', type=bool, default=True, help="Whether to concatenate or average attention head outputs")
    parser.add_argument('--gtr_dropout', type=float, default=0.0, help="Dropout rate for attention coefficients")
    
    # Multiscale-specific arguments
    parser.add_argument('--multiscale_n_mlp_hidden_layers', type=int, default=2,
                        help='Number of hidden layers in MLPs for multiscale models.')
    parser.add_argument('--multiscale_n_mmp_layers', type=int, default=4,
                        help='Number of Multiscale Message Passing layers for multiscale models.')
    parser.add_argument('--multiscale_n_message_passing_layers', type=int, default=2,
                        help='Number of Message Passing layers within each Multiscale Message Passing layer for multiscale models.')

    return parser.parse_args()


# ========== sequence_config.py ==========

# sequence_config.py

import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="Train and evaluate graph-based models on sequence data.")

    # Common arguments
    parser.add_argument('--model', type=str, required=True, choices=[
        'intgnn', 'gcn', 'gat', 'gtr', 'mgn', 
        'gcn-ae', 'gat-ae', 'gtr-ae', 'mgn-ae', 
        'singlescale', 'multiscale', 'multiscale-topk'
    ], help="Model to train.")
    parser.add_argument('--data_keyword', type=str, required=True, help="Common keyword to infer data directories.")
    parser.add_argument('--base_data_dir', type=str, default="/sdf/data/ad/ard/u/tiffan/data/",
                        help="Base directory where the data is stored.")
    parser.add_argument('--base_results_dir', type=str, default="/sdf/data/ad/ard/u/tiffan/results/",
                        help="Base directory where the results are stored.")
    parser.add_argument('--dataset', type=str, required=True, help="Dataset identifier.")
    parser.add_argument('--task', type=str, required=True, choices=['predict_n6d', 'predict_n4d', 'predict_n2d'],
                        help="Task to perform.")
    parser.add_argument('--initial_step', type=int, required=True, help="Index of the initial sequence step.")
    parser.add_argument('--final_step', type=int, required=True, help="Index of the final sequence step.")
    parser.add_argument('--identical_settings', action='store_true',
                        help="Flag indicating whether settings are identical across samples.")
    parser.add_argument('--settings_file', type=str, help="Path to the settings file when identical_settings is True.")
    parser.add_argument('--ntrain', type=int, default=None, help="Number of training examples to use.")
    parser.add_argument('--nepochs', type=int, default=100, help="Number of training epochs.")
    parser.add_argument('--save_checkpoint_every', type=int, default=10, help="Save checkpoint every N epochs.")
    parser.add_argument('--lr', type=float, default=1e-4, help="Learning rate for training.")
    parser.add_argument('--batch_size', type=int, default=8, help="Batch size for training.")
    parser.add_argument('--hidden_dim', type=int, default=64, help="Hidden layer dimension size.")
    parser.add_argument('--num_layers', type=int, default=3, help="Number of layers in the model.")
    parser.add_argument('--pool_ratios', type=float, nargs='+', default=[1.0], help="Pooling ratios for TopKPooling layers.")
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'evaluate'], help="Mode to run.")
    parser.add_argument('--checkpoint', type=str, default=None, help="Path to a checkpoint to resume training.")
    parser.add_argument('--checkpoint_epoch', type=int, default=None, help="Epoch of the checkpoint to load.")
    parser.add_argument('--results_folder', type=str, default=None, help="Directory to save results and checkpoints.")
    parser.add_argument('--random_seed', type=int, default=63, help="Random seed for reproducibility.")
    parser.add_argument('--cpu_only', action='store_true', help="Force the script to use CPU even if GPU is available.")
    parser.add_argument('--verbose', action='store_true', help="Display progress bar while training.")
    parser.add_argument('--subsample_size', type=int, default=None,
                        help="Number of samples to use from the dataset. Use all data if not specified.")

    # Learning Rate Scheduler Arguments
    parser.add_argument('--lr_scheduler', type=str, choices=['none', 'exp', 'lin'], default='none',
                        help="Learning rate scheduler type: 'none', 'exp', or 'lin'.")
    # Exponential Scheduler Parameters
    parser.add_argument('--exp_decay_rate', type=float, default=0.001, help="Decay rate for exponential scheduler.")
    parser.add_argument('--exp_start_epoch', type=int, default=0, help="Start epoch for exponential scheduler.")
    # Linear Scheduler Parameters
    parser.add_argument('--lin_start_epoch', type=int, default=100, help="Start epoch for linear scheduler.")
    parser.add_argument('--lin_end_epoch', type=int, default=1000, help="End epoch for linear scheduler.")
    parser.add_argument('--lin_final_lr', type=float, default=1e-5, help="Final learning rate for linear scheduler.")
    
    # GAT-specific arguments
    parser.add_argument('--gat_heads', type=int, default=1, help="Number of attention heads for GAT layers.")

    # Graph Transformer-specific arguments
    parser.add_argument('--gtr_heads', type=int, default=4, help="Number of attention heads for TransformerConv layers.")
    parser.add_argument('--gtr_concat', type=bool, default=True, help="Whether to concatenate or average attention head outputs.")
    parser.add_argument('--gtr_dropout', type=float, default=0.0, help="Dropout rate for attention coefficients.")
    
    # Multiscale-specific arguments
    parser.add_argument('--multiscale_n_mlp_hidden_layers', type=int, default=2,
                        help='Number of hidden layers in MLPs for multiscale models.')
    parser.add_argument('--multiscale_n_mmp_layers', type=int, default=4,
                        help='Number of Multiscale Message Passing layers for multiscale models.')
    parser.add_argument('--multiscale_n_message_passing_layers', type=int, default=2,
                        help='Number of Message Passing layers within each Multiscale Message Passing layer for multiscale models.')

    return parser.parse_args()


# ========== sequence_train.py ==========

# sequence_train.py

import torch
import torch.nn.functional as F
from src.datasets.sequence_datasets import SequenceGraphDataset
from src.graph_models.models.graph_networks import (
    GraphConvolutionNetwork,
    GraphAttentionNetwork,
    GraphTransformer,
    MeshGraphNet
)
from src.graph_models.models.graph_autoencoders import (
    GraphConvolutionalAutoEncoder,
    GraphAttentionAutoEncoder,
    GraphTransformerAutoEncoder,
    MeshGraphAutoEncoder
)
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.multiscale.gnn import (
    SinglescaleGNN, 
    MultiscaleGNN, 
    TopkMultiscaleGNN
)

from trainers import GraphPredictionTrainer
from sequence_utils import (
    generate_results_folder_name,
    save_metadata,
    get_scheduler,
    set_random_seed
)
from sequence_config import parse_args
from torch.utils.data import Subset
from torch_geometric.loader import DataLoader
import numpy as np
import logging
import re
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def is_autoencoder_model(model_name):
    """
    Determines if the given model name corresponds to an autoencoder.

    Args:
        model_name (str): Name of the model.

    Returns:
        bool: True if it's an autoencoder model, False otherwise.
    """
    return model_name.lower().endswith('-ae') or model_name.lower() in ['multiscale-topk']

if __name__ == "__main__":
    args = parse_args()

    # Set device
    if args.cpu_only:
        device = torch.device('cpu')
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # Generate data directory
    graph_data_dir = os.path.join(args.base_data_dir, args.dataset, f"{args.data_keyword}_graphs")
    logging.info(f"Graph data directory: {graph_data_dir}")

    # Generate results folder name
    if args.results_folder is not None:
        results_folder = args.results_folder
    else:
        results_folder = generate_results_folder_name(args)
    logging.info(f"Results will be saved to {results_folder}")

    # Set random seed
    set_random_seed(args.random_seed)

    # Determine if the model requires edge_attr
    models_requiring_edge_attr = [
        'intgnn', 'gtr', 'mgn', 'gtr-ae', 'mgn-ae', 
        'singlescale', 'multiscale', 'multiscale-topk'
    ]  # Adjusted list
    use_edge_attr = args.model.lower() in models_requiring_edge_attr
    logging.info(f"Model '{args.model}' requires edge_attr: {use_edge_attr}")

    # Initialize dataset
    dataset = SequenceGraphDataset(
        graph_data_dir=graph_data_dir,
        initial_step=args.initial_step,
        final_step=args.final_step,
        task=args.task,
        identical_settings=args.identical_settings,
        settings_file=args.settings_file,
        use_edge_attr=use_edge_attr,
        subsample_size=args.subsample_size
    )

    # Subset dataset if ntrain is specified
    total_dataset_size = len(dataset)
    if args.ntrain is not None:
        np.random.seed(args.random_seed)  # For reproducibility
        indices = np.random.permutation(total_dataset_size)[:args.ntrain]
        dataset = Subset(dataset, indices)
        
    # After initializing the dataset and subset
    for i in range(min(5, len(dataset))):
        sample = dataset[i]
        if hasattr(sample, 'edge_attr') and sample.edge_attr is not None:
            logging.info(f"Sample {i} edge_attr shape: {sample.edge_attr.shape}")
        else:
            logging.warning(f"Sample {i} is missing edge_attr.")

    # Initialize dataloader
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    # Get a sample data for model initialization
    sample = dataset[0]

    # Model initialization
    # Reuse your model initialization code from your old train.py, adjusting as necessary.

    # Example placeholder code (you need to replace this with your actual model initialization code):
    model = GraphConvolutionNetwork(
        in_channels=sample.x.shape[1],
        hidden_dim=args.hidden_dim,
        out_channels=sample.y.shape[1],
        num_layers=args.num_layers,
        pool_ratios=args.pool_ratios,
    )
    logging.info(f"Initialized model: {args.model}")

    model.to(device)
    logging.info(f"Model moved to {device}.")

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    logging.info(f"Initialized Adam optimizer with learning rate: {args.lr}")

    # Scheduler
    scheduler = get_scheduler(args, optimizer)
    if scheduler:
        logging.info("Initialized learning rate scheduler.")

    # Handle checkpoint
    if args.checkpoint is None and args.checkpoint_epoch is not None:
        results_folder_ = re.sub(r'ep\d+', f'ep{args.checkpoint_epoch}', results_folder)
        checkpoint_path = os.path.join(results_folder_, 'checkpoints', f'model-{args.checkpoint_epoch - 1}.pth')
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint for epoch {args.checkpoint_epoch} not found at {checkpoint_path}. Exiting.")
            exit(1)
        else:
            args.checkpoint = checkpoint_path
            logging.info(f"Checkpoint set to: {args.checkpoint}")

    if args.checkpoint is not None and args.checkpoint_epoch is None:
        checkpoint_path = args.checkpoint
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint not found at {checkpoint_path}. Exiting.")
            exit(1)  
        else:
            logging.info(f"Checkpoint set to: {checkpoint_path}")

    # Define the loss function
    criterion = torch.nn.MSELoss()

    # Initialize trainer with the loss function
    trainer = GraphPredictionTrainer(
        model=model,
        dataloader=dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        nepochs=args.nepochs,
        save_checkpoint_every=args.save_checkpoint_every,
        results_folder=results_folder,
        checkpoint=args.checkpoint,
        random_seed=args.random_seed,
        device=device,
        verbose=args.verbose,
        criterion=criterion
    )
    logging.info("Initialized GraphPredictionTrainer with custom loss function.")
    
    # Save metadata
    save_metadata(args, model, results_folder)

    # Run train or evaluate
    if args.mode == 'train':
        trainer.train()
    else:
        # Implement evaluation if needed
        logging.info("Evaluation mode is not implemented yet.")
        pass


# ========== sequence_train_accelerate.py ==========

# sequence_train_accelerate.py

import torch
import torch.nn.functional as F
from src.datasets.sequence_datasets import SequenceGraphDataset
from src.graph_models.models.graph_networks import (
    GraphConvolutionNetwork,
    GraphAttentionNetwork,
    GraphTransformer,
    MeshGraphNet
)
from src.graph_models.models.graph_autoencoders import (
    GraphConvolutionalAutoEncoder,
    GraphAttentionAutoEncoder,
    GraphTransformerAutoEncoder,
    MeshGraphAutoEncoder
)
from src.graph_models.models.intgnn.models import GNN_TopK
from src.graph_models.models.multiscale.gnn import (
    SinglescaleGNN, 
    MultiscaleGNN, 
    TopkMultiscaleGNN
)

from trainers_accelerate import GraphPredictionTrainer
from sequence_utils import (
    generate_results_folder_name,
    save_metadata,
    get_scheduler,
    set_random_seed
)
from sequence_config import parse_args
from torch.utils.data import Subset
from torch_geometric.loader import DataLoader
import numpy as np
import logging
import re
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def is_autoencoder_model(model_name):
    """
    Determines if the given model name corresponds to an autoencoder.

    Args:
        model_name (str): Name of the model.

    Returns:
        bool: True if it's an autoencoder model, False otherwise.
    """
    return model_name.lower().endswith('-ae') or model_name.lower() in ['multiscale-topk']

if __name__ == "__main__":
    args = parse_args()

    # Set device
    if args.cpu_only:
        device = torch.device('cpu')
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # Generate data directory
    graph_data_dir = os.path.join(args.base_data_dir, args.dataset, f"{args.data_keyword}_graphs")
    logging.info(f"Graph data directory: {graph_data_dir}")

    # Generate results folder name
    if args.results_folder is not None:
        results_folder = args.results_folder
    else:
        results_folder = generate_results_folder_name(args)
    logging.info(f"Results will be saved to {results_folder}")

    # Set random seed
    set_random_seed(args.random_seed)

    # Determine if the model requires edge_attr
    models_requiring_edge_attr = [
        'intgnn', 'gtr', 'mgn', 'gtr-ae', 'mgn-ae', 
        'singlescale', 'multiscale', 'multiscale-topk'
    ]  # Adjusted list
    use_edge_attr = args.model.lower() in models_requiring_edge_attr
    logging.info(f"Model '{args.model}' requires edge_attr: {use_edge_attr}")

    # Initialize dataset
    dataset = SequenceGraphDataset(
        graph_data_dir=graph_data_dir,
        initial_step=args.initial_step,
        final_step=args.final_step,
        task=args.task,
        identical_settings=args.identical_settings,
        settings_file=args.settings_file,
        use_edge_attr=use_edge_attr,
        subsample_size=args.subsample_size
    )

    # Subset dataset if ntrain is specified
    total_dataset_size = len(dataset)
    if args.ntrain is not None:
        np.random.seed(args.random_seed)  # For reproducibility
        indices = np.random.permutation(total_dataset_size)[:args.ntrain]
        dataset = Subset(dataset, indices)
        
    # After initializing the dataset and subset
    for i in range(min(5, len(dataset))):
        sample = dataset[i]
        if hasattr(sample, 'edge_attr') and sample.edge_attr is not None:
            logging.info(f"Sample {i} edge_attr shape: {sample.edge_attr.shape}")
        else:
            logging.warning(f"Sample {i} is missing edge_attr.")

    # Initialize dataloader
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    # Get a sample data for model initialization
    sample = dataset[0]

    # Model initialization
    # Reuse your model initialization code from your old train.py, adjusting as necessary.

    # Example placeholder code (you need to replace this with your actual model initialization code):
    model = GraphConvolutionNetwork(
        in_channels=sample.x.shape[1],
        hidden_dim=args.hidden_dim,
        out_channels=sample.y.shape[1],
        num_layers=args.num_layers,
        pool_ratios=args.pool_ratios,
    )
    logging.info(f"Initialized model: {args.model}")

    model.to(device)
    logging.info(f"Model moved to {device}.")

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    logging.info(f"Initialized Adam optimizer with learning rate: {args.lr}")

    # Scheduler
    scheduler = get_scheduler(args, optimizer)
    if scheduler:
        logging.info("Initialized learning rate scheduler.")

    # Handle checkpoint
    if args.checkpoint is None and args.checkpoint_epoch is not None:
        results_folder_ = re.sub(r'ep\d+', f'ep{args.checkpoint_epoch}', results_folder)
        checkpoint_path = os.path.join(results_folder_, 'checkpoints', f'model-{args.checkpoint_epoch - 1}.pth')
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint for epoch {args.checkpoint_epoch} not found at {checkpoint_path}. Exiting.")
            exit(1)
        else:
            args.checkpoint = checkpoint_path
            logging.info(f"Checkpoint set to: {args.checkpoint}")

    if args.checkpoint is not None and args.checkpoint_epoch is None:
        checkpoint_path = args.checkpoint
        if not os.path.exists(checkpoint_path):
            logging.error(f"Checkpoint not found at {checkpoint_path}. Exiting.")
            exit(1)  
        else:
            logging.info(f"Checkpoint set to: {checkpoint_path}")

    # Define the loss function
    criterion = torch.nn.MSELoss()

    # Initialize trainer with the loss function
    trainer = GraphPredictionTrainer(
        model=model,
        dataloader=dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        nepochs=args.nepochs,
        save_checkpoint_every=args.save_checkpoint_every,
        results_folder=results_folder,
        checkpoint=args.checkpoint,
        random_seed=args.random_seed,
        device=device,
        verbose=args.verbose,
        criterion=criterion
    )
    logging.info("Initialized GraphPredictionTrainer with custom loss function.")
    
    # Save metadata
    save_metadata(args, model, results_folder)

    # Run train or evaluate
    if args.mode == 'train':
        trainer.train()
    else:
        # Implement evaluation if needed
        logging.info("Evaluation mode is not implemented yet.")
        pass
